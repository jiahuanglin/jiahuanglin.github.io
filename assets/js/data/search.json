[ { "title": "React, redux and hooks", "url": "/posts/react-redux-and-hooks/", "categories": "Software", "tags": "framework, frontend", "date": "2022-07-09 15:58:00 -0400", "snippet": "React &amp; React hooksClass Component vs Functional ComponentThe following is an example of writing a class component in React:class Welcome extends React.Component { render() { return &lt;h1&gt;Hello, {this.props.name}&lt;/h1&gt;; }}The React team hopes that components don’t become complex containers but preferably just pipelines for data flow. The best way to write components should be functions, not classes, like the following:function Welcome(props) { return &lt;h1&gt;{props.name}&lt;/h1&gt;;}However, there are significant limitations to this writing style; it must be a pure function, cannot contain state, and does not support lifecycle methods, so it cannot replace classes.React Hooks are designed to be an enhanced version of functional components, writing a fully functional component without using “classes” at all. In another words, components should be written as purely functional as possible, and if external functionality and side effects are needed, external code is “hooked” in with hooks. React Hooks are those hooks.React provides some common hooks by default, but you can also wrap your own hooks.All hooks introduce external functionality to the function, so React makes it a convention to name all hooks with the use prefix to make them easy to identify. If you want to use the xxx function, the hook will be named usexxx.The following are some of the most common hooks that React provides by default.useState()useContext()useReducer()useEffect()useEffect()useEffect() itself is a function, provided by the React framework, that can be called from within the function component.For example, we want the page title (document.title) to change when the component is loaded. Then, the action of changing the page title is a side effect of the component, and must be implemented with useEffect().import React, { useEffect } from 'react';function Welcome(props) { useEffect(() =&gt; { document.title = 'Loading complete'; }); return &lt;h1&gt;{props.name}&lt;/h1&gt;;}In the above example, the argument to useEffect() is a function that is the side effect (changing the page title) to be completed. Once the component is loaded, React executes this function.What useEffect() does is to specify a side effect function that is automatically executed every time the component is rendered. The side effect function is also executed after the component is loaded in the page DOM for the first time.Sometimes we don’t want useEffect() to be executed every time it renders, so we can use its 2nd argument to specify the dependencies of the side effect function using an array, and only if the dependencies change will it be re-rendered.function Welcome(props) { useEffect(() =&gt; { document.title = `Hello, ${props.name}`; }, [props.name]); return &lt;h1&gt;{props.name}&lt;/h1&gt;;}In the above example, the second argument to useEffect() is an array specifying the dependency (props.name) of the first argument (the side-effect function). The side effect function will be executed only if this variable changes.If the second argument is an empty array, the side effect parameter has no dependencies. Therefore, the side effect function will only be executed once the component is loaded into the DOM at this point, but never not again whenever the component re-renders. This is because the side effect does not depend on any variables, so no matter how those variables change, the result of the side effect function will not change, so running it once is enough.Hook DependenciesHooks allows you to listen for a change in data. This change may trigger a refresh of a component, create a side effect, or refresh a cache. Defining which data changes to listen for is a matter of specifying the Hooks’ dependencies. However, it is important to note that dependencies are not a special mechanism of built-in Hooks, but can be considered as a design pattern. Hooks that have similar requirements can be implemented using this pattern. When defining a dependency, we need to pay attention to the following three points: The variable defined in the dependency must be used in the callback function. Otherwise, it is meaningless to declare dependency. A dependency is generally an array of constants, not a variable. React uses shallow comparisons to compare whether dependencies have changed, so pay special attention to arrays or object types. If you are creating a new object each time, even if it is equivalent to the previous value, it will assume that the dependencies have changed. This area can easily lead to bugs when you first start using Hooks. For example, the following code:function Sample() { // Here a new array is created each time the component is executed const todos = [text: 'Learn hooks.'}]; useEffect(() =&gt; { console.log('Todos changed.'); }, [todos] );}The code’s original intent was to generate side effects when todos change. Still, here the todos variable is created within the function, effectively generating a new array each time. So the comparison of references made while acting as dependencies is considered to have changed.ReduxRedux is a useful architecture, but it’s not a must-have. In fact, for the most part, you can get away without it and use React. “If you don’t know if you need Redux, you don’t need it.” “You only need Redux if you encouter a problem that React just can’t solve.” Dan Abramov, the creator of ReduxSimply Put, if your UI layer is straightforward and doesn’t have a lot of interaction, Redux is unnecessary and adds complexity. Complex user usage Users with different identities have different usage styles (e.g., regular users and administrators) Multiple users can collaborate There is a lot of interaction with the server, or WebSockets are used View has to get data from multiple sourcesThese are the situations where Redux is appropriate: multiple interactions, multiple data sources.From a component perspective, consider using Redux if your application has the following scenarios A component has state that needs to be shared A piece of state needs to be available anywhere A component needs to change global state A component needs to change the state of another componentIn the above scenario, the code will quickly become a mess if you don’t use Redux or other state management tools and don’t handle reading and writing state according to certain rules. You need a mechanism to query state, change state, and propagate state changes simultaneously.StoreStore is the place where data is stored, you can think of it as a container. There can be only one Store for the whole application.Redux provides the createStore function, which is used to generate a Store.import { createStore } from 'redux';const store = createStore(fn);In the above code, the createStore function accepts another function as an argument and returns the newly generated Store object.StateThe Store object contains all the data. If you want to get the data at a certain point in time, you have to generate a snapshot of the Store. This collection of data at a point in time is called a State.The State of the current moment can be obtained by store.getState().import { createStore } from 'redux';const store = createStore(fn);const state = store.getState();Redux specifies that a State corresponds to a View, and as long as the State is the same, the View is the same. If you know the State, you know what the View is, and vice versa.ActionChanges to the State will result in changes to the View. Action is a notification from the View that the State is supposed to change.Action is an object. The type attribute is required and indicates the name of the Action. The other properties can be set freely, and the community has a specification to follow.const action = { type: 'ADD_TODO', payload: 'Learn Redux'};In the above code, the name of the Action is ADD_TODO and the information it carries is the string Learn Redux.As you can understand, the Action describes what is currently happening. The only way to change the state is to use the Action, which transports data to the Store." }, { "title": "Implementing a simple RPC framework", "url": "/posts/implementing-a-simple-rpc-framework/", "categories": "Software", "tags": "RPC, framework, server, serialization", "date": "2022-06-28 17:11:00 -0400", "snippet": "gRPC Examplesyntax = \"proto3\";option java_multiple_files = true;option java_package = \"io.grpc.hello\";option java_outer_classname = \"HelloProto\";option objc_class_prefix = \"HLW\";package hello;service HelloService{ rpc Say(HelloRequest) returns (HelloReply) {}}message HelloRequest { string name = 1;}message HelloReply { string message = 1;}Client.javapackage io.grpc.hello;import io.grpc.ManagedChannel;import io.grpc.ManagedChannelBuilder;import io.grpc.StatusRuntimeException;import java.util.concurrent.TimeUnit;public class HelloWorldClient { private final ManagedChannel channel; private final HelloServiceGrpc.HelloServiceBlockingStub blockingStub; /** * construct network channel **/ public HelloWorldClient(String host, int port) { this(ManagedChannelBuilder.forAddress(host, port) .usePlaintext() .build()); } /** * build stub for sending request **/ HelloWorldClient(ManagedChannel channel) { this.channel = channel; blockingStub = HelloServiceGrpc.newBlockingStub(channel); } /** * resource destruction **/ public void shutdown() throws InterruptedException { channel.shutdown().awaitTermination(5, TimeUnit.SECONDS); } /** * send rpc request **/ public void say(String name) { HelloRequest request = HelloRequest.newBuilder().setName(name).build(); HelloReply response; try { response = blockingStub.say(request); } catch (StatusRuntimeException e) { return; } System.out.println(response); } public static void main(String[] args) throws Exception { HelloWorldClient client = new HelloWorldClient(\"127.0.0.1\", 50051); try { client.say(\"world\"); } finally { client.shutdown(); } }}Server.javapackage io.grpc.hello;import io.grpc.Server;import io.grpc.ServerBuilder;import io.grpc.stub.StreamObserver;import java.io.IOException;public class HelloWorldServer { static class HelloServiceImpl extends HelloServiceGrpc.HelloServiceImplBase { @Override public void say(HelloRequest req, StreamObserver&lt;HelloReply&gt; responseObserver) { HelloReply reply = HelloReply.newBuilder().setMessage(\"Hello \" + req.getName()).build(); responseObserver.onNext(reply); responseObserver.onCompleted(); }} private Server server; /** * expose service **/ private void start() throws IOException { int port = 50051; server = ServerBuilder.forPort(port) .addService(new HelloServiceImpl()) .build() .start(); Runtime.getRuntime().addShutdownHook(new Thread() { @Override public void run() { HelloWorldServer.this.stop(); } }); } /** * resource destruction **/ private void stop() { if (server != null) { server.shutdown(); } } /** * resource destruction **/ private void blockUntilShutdown() throws InterruptedException { if (server != null) { server.awaitTermination(); } } public static void main(String[] args) throws IOException, InterruptedException { final HelloWorldServer server = new HelloWorldServer(); server.start(); server.blockUntilShutdown(); } }Dynamic proxyRPC will automatically generate a proxy class for the interface.When we inject the interface into the project, the actual binding process is the proxy class generated by the interface. This way, when the interface method is called, it is intercepted by the generated proxy class so that we can add remote call logic to the generated proxy class. By this technique, we can help users to shield the details of the remote call, and achieve the experience of calling the remote as if it were a local call.Let’s take a look at the following example:public interface Hello { String say();}public class RealHello { public String invoke(){ return \"I'm proxy\"; }}// Autogenerated MyProxy class hides the details of remote call into RealHellopublic class MyProxy implements InvocationHandler { private Object target; MyProxy(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] paramValues) { return ((RealHello)target).invoke(); }}public class TestProxy { public static void main(String[] args){ MyProxy proxy = new MyProxy(new RealHello()); ClassLoader classLoader = ClassLoaderUtils.getCurrentClassLoader(); // this line binds the Hello interface to the MyProxy Hello test = (Hello) Proxy.newProxyInstance(classLoader, new Class[]{Hello.class}, proxy); System.out.println(test.say()); // this prints \"I'm proxy\" }}Implementing a frameworkpublic class RpcRequest { private String requestId; private String className; private String methodName; private Class&lt;?&gt;[] parameterTypes; private Object[] parameters; // getter/setter...}public class RpcResponse { private String requestId; private Throwable error; private Object result; // getter/setter...}Serializaion (use Hessian)import com.caucho.hessian.io.HessianInput;import com.caucho.hessian.io.HessianOutput;import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.IOException;public class Serializer { public static &lt;T&gt; byte[] serialize(T input) throws IOException { ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); HessianOutput hessianOutput = new HessianOutput(byteArrayOutputStream); hessianOutput.writeObject(input); return byteArrayOutputStream.toByteArray(); } public static &lt;T&gt; T deserialize(byte[] input) throws IOException { ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(input); HessianInput hessianInput = new HessianInput(byteArrayInputStream); return (T) hessianInput.readObject(); }}Service discoveryRequest handlepublic class RequestHandler extends SimpleChannelInboundHandler&lt;RpcRequest&gt; { private static final Logger LOGGER = LoggerFactory.getLogger(RpcHandler.class); private Map&lt;String, Object&gt; serviceMap; public RequestHandler(Map&lt;String, Object&gt; serviceMap) { this.serviceMap = serviceMap; } @Override public void channelRead0(final ChannelHandlerContext ctx, RpcRequest request) throws Exception { RpcResponse response = new RpcResponse(); response.setRequestId(request.getRequestId()); try { Object result = handle(request); response.setResult(result); } catch (Throwable t) { response.setError(t); } ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); } private RpcResponse handle(RpcRequest request) { RpcResponse response = new RpcResponse(); response.setId(request.getId()); Object instance = serviceMap.get(request.getInterfaceName()); if (instance == null) { return null; } try { Class&lt;?&gt;[] types = Arrays.stream(request.getArguments()) .map(Argument::getType) .toArray(Class&lt;?&gt;[]::new); Method method = instance.getClass() .getMethod(request.getMethodName(), types); Object[] args = Arrays.stream(request.getArguments()) .map(Argument::getValue) .toArray(Object[]::new); Object result = method.invoke(instance, args); response.setResponse(result); } catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException e) { return null; } return response; } @Override public void userEventTriggered(ChannelHandlerContext ctx, Object event) throws Exception { if (event instanceof IdleStateEvent) { IdleStateEvent e = (IdleStateEvent) event; if (e.state() == IdleState.READER_IDLE) { ctx.channel().close(); } } else { super.userEventTriggered(ctx, event); } } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) { LOGGER.error(\"server caught exception\", cause); ctx.close(); } ...}RPC servicepublic class RpcServer implements ApplicationContextAware, InitializingBean { private static final Logger LOGGER = LoggerFactory.getLogger(RpcServer.class); private String serverAddress; private ServiceRegistry serviceRegistry; private Map&lt;String, Object&gt; handlerMap = new HashMap&lt;&gt;(); public RpcServer(String serverAddress) { this.serverAddress = serverAddress; } public RpcServer(String serverAddress, ServiceRegistry serviceRegistry) { this.serverAddress = serverAddress; this.serviceRegistry = serviceRegistry; } @Override public void setApplicationContext(ApplicationContext ctx) throws BeansException { if (MapUtils.isNotEmpty(serviceBeanMap)) { for (Object serviceBean : serviceBeanMap.values()) { String interfaceName = serviceBean.getClass().getAnnotation(RpcService.class).value().getName(); handlerMap.put(interfaceName, serviceBean); } } } @Override public void afterPropertiesSet() throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel channel) throws Exception { channel.pipeline() .addLast(new RpcDecoder(RpcRequest.class)) .addLast(new RpcEncoder(RpcResponse.class)) .addLast(new RequestHandler(handlerMap)); } }) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); String[] array = serverAddress.split(\":\"); String host = array[0]; int port = Integer.parseInt(array[1]); ChannelFuture future = bootstrap.bind(host, port).sync(); LOGGER.debug(\"server started on port {}\", port); if (serviceRegistry != null) { serviceRegistry.register(serverAddress); } future.channel().closeFuture().sync(); } finally { workerGroup.shutdownGracefully(); bossGroup.shutdownGracefully(); } }}Load balancingpublic class RoundRobinLoadBalancer implements LoadBalancer { private AtomicInteger current = new AtomicInteger(0); @Override public RpcProtocol route(String serviceKey, Map&lt;Connection, RpcClient&gt; connectedServerNodes) throws Exception { Map&lt;String, List&lt;Connection&gt;&gt; serviceMap = getServiceMap(connectedServerNodes); List&lt;Connection&gt; addressList = serviceMap.get(serviceKey); if (addressList != null &amp;&amp; addressList.size() &gt; 0) { int size = addressList.size(); int index = (current.getAndAdd(1) + size) % size; return addressList.get(index); } else { throw new Exception(\"Can not find connection for service: \" + serviceKey); } }}Dynamic proxypublic class RpcProxy { private String serverAddress; private ServiceDiscovery serviceDiscovery; public RpcProxy(String serverAddress) { this.serverAddress = serverAddress; } public RpcProxy(ServiceDiscovery serviceDiscovery) { this.serviceDiscovery = serviceDiscovery; } public &lt;T&gt; T create(Class&lt;?&gt; interfaceClass) { return (T) Proxy.newProxyInstance( interfaceClass.getClassLoader(), new Class&lt;?&gt;[]{interfaceClass}, new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { RpcRequest request = new RpcRequest(); request.setRequestId(UUID.randomUUID().toString()); request.setClassName(method.getDeclaringClass().getName()); request.setMethodName(method.getName()); request.setParameterTypes(method.getParameterTypes()); request.setParameters(args); if (serviceDiscovery != null) { serverAddress = serviceDiscovery.discover(); } String[] array = serverAddress.split(\":\"); String host = array[0]; int port = Integer.parseInt(array[1]); RpcClient client = new RpcClient(host, port); RpcResponse response = client.send(request); if (response.isError()) { throw response.getError(); } else { return response.getResult(); } } } ); }}Clientpublic class RpcClient extends SimpleChannelInboundHandler&lt;RpcResponse&gt; { private static final Logger LOGGER = LoggerFactory.getLogger(RpcClient.class); private String host; private int port; private RpcResponse response; private final Object obj = new Object(); public RpcClient(String host, int port) { this.host = host; this.port = port; } @Override public void channelRead0(ChannelHandlerContext ctx, RpcResponse response) throws Exception { this.response = response; synchronized (obj) { obj.notifyAll(); } } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { LOGGER.error(\"client caught exception\", cause); ctx.close(); } public RpcResponse send(RpcRequest request) throws Exception { EventLoopGroup group = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group).channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override public void initChannel(SocketChannel channel) throws Exception { channel.pipeline() .addLast(new RpcEncoder(RpcRequest.class)) .addLast(new RpcDecoder(RpcResponse.class)) .addLast(RpcClient.this); } }) .option(ChannelOption.SO_KEEPALIVE, true); ChannelFuture future = bootstrap.connect(host, port).sync(); future.channel().writeAndFlush(request).sync(); synchronized (obj) { obj.wait(); } if (response != null) { future.channel().closeFuture().sync(); } return response; } finally { group.shutdownGracefully(); } }}" }, { "title": "Faster database queries", "url": "/posts/faster-database-queries/", "categories": "System", "tags": "database, cache", "date": "2022-06-17 11:23:00 -0400", "snippet": "CacheMost production systems use the classic combination of MySQL and Redis. Redis acts as a front-end cache for MySQL, blocking most of the query requests for MySQL and essentially relieving the pressure on MySQL’s concurrent requests.Redis is a high-performance KV database that uses memory to store data, and its high performance comes from its simple data structure and use of memory to store data. By design, Redis sacrifices much of its functionality and data reliability for high performance. But it is these same features that make Redis particularly suitable as a front-end cache for MySQL. Although Redis supports persisting data to disk and master-slave replication, you need to know that Redis is still an unreliable store and is not designed to be reliable. We generally use Redis as a cache.Even if Redis is only used as a cache, we must design the Redis cache with its data unreliability characteristic in mind, or in other words, our application must be compatible with Redis data loss when using Redis, so that even if Redis data loss occurs, it does not affect the data accuracy of the system.When caching a MySQL table, the primary key is usually used in Redis. For example, if you are caching an order table, you would use the order number as the primary key in Redis. Value is used to store the entire order record after serialization. Then we come to the question of how to update the data in the cache. Read/Write Through: When querying order data, first go to the cache query. If the cache is hit, then directly return the order data. If it does not hit, then go to the database query, get the query results after the order data into the cache, and then return. When updating the order data, we first update the order table in the database and then update the data in the cache if the update is successful.Is there any problem with using the cache in the above way? Probably not in most cases. However, in the case of concurrency, there is a certain probability that dirty data will occur, and the data in the cache may be incorrectly updated with old data. For example, for the same order record, a read request and a write request are generated at the same time, and these two requests are assigned to two different threads to execute in parallel, the read thread tries to read the cache and misses, and goes to the database to read the order data, then maybe another read thread updates the cache first, and in the thread that handles the write request, it updates the data and the cache, then the first read thread with the old order data updates the cache to the old data.So how to solve the dirty data problem? The Cache Aside pattern is very similar to the Read/Write Through pattern above, but with only a small difference: The Cache Aside pattern does not try to update the cache when updating data, but to delete the cache. After the order service receives a request to update data, it first updates the database, and if the update is successful, then tries to delete the order in the cache. This updated order data will be loaded into the cache the next time it is accessed. Using Cache Aside mode to update the cache is very effective in avoiding dirty data problems caused by concurrent reads and writes.Read-write separationHowever, caching is not so effective for user-related systems, such as order systems, account systems, shopping cart systems, and so on. In these systems, each user needs to query information related to the user. Even if it is the same function interface, the data that each person sees is different. For example, in the “My Orders” function, where users see their order data, I open my order cache data, but can not open your order for you to use because our two orders are different. In this case, the cache’s hit rate is not that high, and there are still a considerable number of query requests that hit MySQL because they do not hit the cache. Then, as the number of system users grows, more and more read and write requests hit MySQL. What do we do when a single MySQL cannot support many concurrent requests?When a single MySQL cannot meet the requirements, multiple MySQL instances can only be used to take up the large number of read and write requests. MySQL, like most commonly used relational databases, is typically a standalone database and does not support distributed deployment. It is very difficult to use multiple instances of a single database to form a cluster and provide distributed database services. It requires a lot of extra work when deploying a cluster, and it is hard to be transparent to the application then your application has to make a big architectural adjustment for that too. So, unless the system is really large enough that this is the only way to go, it is not recommended that you shard your data and build MySQL clusters on your own, which is very costly. A simple and very effective solution is not to slice the data, but to use multiple MySQL instances with the same data to share many query requests, often called read-write separation.The reason read-write separation can solve the problem is that it is based on an objective situation favourable to us. Many systems, especially systems for public users, have a severe imbalance in the ratio of read-to-write data. The read-to-write ratio is generally around a few dozen, with an average of only one update request for every few dozen query requests. In other words, the vast majority of requests the database needs to respond to are read-only query requests. A distributed storage system is complicated to do distributed writes because it is challenging to solve the data consistency problem. As long as I can synchronize the data to these read-only instances in real-time and ensure that the data on these read-only instances are the same, these read-only instances can share many query requests. Another benefit of read-write separation is that it is relatively easy to implement. Upgrading a system using standalone MySQL to a read-write separated multi-instance architecture is very easy and generally requires no changes to the business logic of the system, just simple changes to the DAO code to separate the read and write requests to the database.Query optimizationSharding" }, { "title": "Code refactoring", "url": "/posts/code-refactoring/", "categories": "Software", "tags": "refactoring", "date": "2022-06-05 11:23:00 -0400", "snippet": " Refactoring is the improvement of the internal structure of software without changing its observable behavior.When to refactorWhen adding new featuresThe most common time to refactor is when I want to add a new feature to the software.At this point, the immediate reason for refactoring is often to help me understand the code that needs to be changed - code that may have been written by someone else or by me. Part of the reason for doing this is to make the code easier to understand the next time I look at it, but the main reason is this: I can understand more from it if I get the structure of the code straightened out as I move forward.Another motivation for refactoring here is the code’s design doesn’t help me easily add the features I need. Part of the reason for doing this is to make it easier to add new features in the future, but the main reason is the same: I’ve found it to be the quickest route.Refactoring is a fast and fluid process, and once it’s done, new features are added more quickly and smoothly.When fixing bugsMost of the time, refactoring is used during debugging to make the code more readable.As I look at the code and try to understand it, I use refactoring to help deepen my understanding.I find that working with the code in this way often helps me to identify bugs.Think of it this way: if you get a bug report, that’s a sign that you need to refactor because the code isn’t clear enough - not clear enough to see the bug at a glance.When reviewing codeCode reviews are crucial for writing clear code. Refactoring helps us review other people’s codes as well as the code review process to get more concrete results.My code may be apparent to me but not to others. This is unavoidable because it is difficult to get developers to put themselves in the shoes of people unfamiliar with what they are doing.Code review also gives more people the opportunity to make useful suggestions; after all, there are only so many good ideas one can come up with in a week.EncapsulationSimplify conditional logicDecompose conditional logicFromif (!date.isBefore(subscriptionStartDate) &amp;&amp; !date.isAfter(subscriptionEndDate)) { // passed premissiong check, proceed validSubscriptionHandler.handle(...); ...} else { invalidSubscriptionHandler.handle(...); ...} toif (isValidSubscription(date)) { // passed premissiong check, proceed validSubscriptionHandler.handle(...); ...} else { invalidSubscriptionHandler.handle(...); ...} For conditional logic, breaking each branch condition into new functions has additional benefits: it highlights the conditional logic, clarifies what each branch does, and highlights the reason for each branch.Consolidate Conditional ExpressionSometimes we will find a string of conditional checks that check for different conditions but end up with the same behaviour.if (employee.isJunior) return 0;if (employee.monthsEmployed &lt; 12) return 0;if (employee.isPartTime) return 0;Combine sequentially executed conditional expressions with logical or, and, not:function isEligibleForPromotion() { return ((employee.isJunior) || (employee.monthsEmployed &lt;&gt; 12) || (employee.isPartTime));}Replace Nested Conditional with Guard ClausesFromfunction getPayAmount() { int result; if (isSenior) result = seniorAmount(); else { if (isRemote) result = remoteAmount(); else { if (isRetired) result = retiredAmount(); else result = normalPayAmount(); } } return result;}tofunction getPayAmount() { if (isSenior) return seniorAmount(); if (isRemote) return remoteAmount(); if (isRetired) return retiredAmount(); return normalPayAmount();}Replace Conditional with PolymorphismFromfunction getPayAmount() { if (isSenior) return seniorAmount(); if (isRemote) return remoteAmount(); if (isRetired) return retiredAmount(); return normalPayAmount();}toclass Employee { getSalary() { return salary; }}class SeniorEmployee extends Employee { ...}class RemoteEmployee extends Employee { ...}class RetiredEmployee extends Employee { ...}function getPayAmount(Employee e) { return e.getSalary();}Reorganize data" }, { "title": "How to design a distributed cache system?", "url": "/posts/how-to-design-a-cache-system/", "categories": "System", "tags": "system design, cache system", "date": "2022-05-26 02:45:00 -0400", "snippet": " The following are my notes from reading G.K’s system design bookHow to design a caching system?The caching system is a widely used technology in almost all applications today. In addition, it applies to every layer of the technology stack. For example, DNS lookups heavily utilize caching.In short, a caching system (possibly in memory) stores commonly used resources so that the next request of the same resource can return immediately. It improves system efficiency by consuming more storage space.LRUOne of the most commonly used caching systems is the LRU (longest unused). The way an LRU cache works is very simple. When a client requests resource A, the following happens: If A exists in the cache, we return it immediately. If not, and the cache has additional storage, we fetch resource A and return it to the client. Alternatively, A is inserted into the cache. If the cache is full, we eliminate the longest unused resource and replace it with resource AThe strategy here is to maximize the requested resource’s chances in the cache. So how can we implement a simple LRU?LRU DesignThe LRU cache should support these operations: lookup, insertion and deletion. To achieve fast lookups, we need to use hashing. By the same token, if we want to insert/delete quickly, a list of links comes to mind. Since we need to find the longest unused items efficiently, we need to order the queues, stacks, or ordered arrays.We can use a queue implemented by a two-way linked list to store all the resources to combine all these analyses. In addition, a hash table is needed, where the resource identifier is the key and the address of the corresponding queue node is the value.The working principle is, when requesting resource A, we check the hash table to see if A exists in the cache. if so, we can immediately find the corresponding queue node, return the resource (and move it to the end of the queue). If not, we add A to the cache. If there is enough space, we simply add A to the end of the queue. Otherwise, we need to delete the longest unused entry. We can easily delete the head of the queue and the corresponding entry in the hash table to do this. The resource IDs also need to be stored in the queue, so that after removing resources from the queue, they can be removed from the hash table as well.Page change policyWhen the cache is full, we need to delete the existing items to store the new resources. In fact, deleting the oldest unused items is only one of the most common methods. So are there other ways to do it? Random Replacement (RR) - As the term suggests, we can remove an entry at random. Least Frequently Used (LFU) - We maintain the frequency of requests for each item and remove the least frequently used items. W-TinyLFU - This is a modern page-changing strategy. In a nutshell, the problem with LFU is that sometimes an item is only used frequently in the past, and LFU still keeps the item for a long time. W-TinyLFU solves this problem by calculating the frequency within a time window. It also has various storage optimizations.ConcurrencyCaches will have concurrency problems as it forms a classic read-write problem. When multiple clients try to update the cache simultaneously, there may be conflicts. For example, two clients may compete for the same cache slot, and the last client to update the cache will win.Of course, the standard solution is to use locks. The disadvantage is obvious - it can seriously affect performance. How can we optimize it?One way is to split the cache into multiple slices and assign a lock to each slice so that if clients update the cache in different slices, they don’t wait for each other. However, since popular entries are more likely to be accessed, some shards will lock more frequently than others.Another approach is to use commit logs. We can store all changes in the log to update the cache instead of updating them immediately. Then some background process will execute all the logs asynchronously. This strategy is usually used in database design.Distributed cachingWhen the system reaches a certain size, we need to distribute the cache to multiple machines.The general strategy is to keep a hash table that maps each resource to the corresponding machine. Thus, when a resource A is requested, from this hash table, we know that machine M is responsible for caching A and directing the request to M. In machine M, it works similarly to the local cache discussed above. If A does not exist in memory, machine M may need to fetch and update A’s cache. After that, it returns the cache to the original. After that, it returns the cache to the original server." }, { "title": "How does browser render a page", "url": "/posts/how-does-browser-render-a-page/", "categories": "System", "tags": "browser, frontend", "date": "2022-05-12 11:23:00 -0400", "snippet": "HTML, CSS and JavaScriptHTML is made up of tags and text. Each tag has its semantic meaning, and the browser will display the HTML content correctly according to the semantic meaning of the tag.&lt;p&gt; Hello World&lt;/p&gt;For example, the above tag is telling the browser that the content here needs to create a new paragraph, and the text in the middle is what needs to be displayed in the section. Suppose you need to change the font colour, size, and other information in the HTML. In that case, you need to use CSS, which is also known as a cascading style sheet and consists of selectors and properties, such as the p selector in the code, which selects the contents of the HTML tag, and then applies the selector’s property values to the content of the tag. There is a colour attribute inside the selector, and its value is red, which tells the rendering engine to display the tag’s contents as red.As for JavaScript, you can use it to make the content of a web page “move” or “dynamically changing”. For example, you can use JavaScript to modify the CSS style value to change the text color.Rendering pipelineBecause of the complexity of the rendering mechanism, the rendering module is divided into many sub-stages during its execution, and the input HTML passes through these sub-stages before the final output pixels. We call such a processing flow a rendering pipeline.The pipeline can be divided into sub-stages according to the rendering chronology: building DOM tree style calculation layout stage layering drawing chunking rasterization and compositing Building a DOM treeWhy do we build a DOM tree? This is because browsers cannot understand and use HTML directly, so they need to convert HTML into a structure that browsers can understand - a DOM tree.Generating the DOM tree is just a first step, up to now we still don’t know the style of the DOM nodes. Making the DOM nodes have the correct style requires style calculation.Styling &amp;&amp; layoutThe purpose of recalculate style is to calculate the specific style of each element in the DOM node. This stage can be divided into three steps to complete. Convert CSS into a structure that the browser can understand. Like HTML files, browsers cannot understand these plain text CSS styles directly, so when the rendering engine receives the CSS text, it performs a conversion operation that converts the CSS text into a structure that the browser can understand - styleSheets. The rendering engine will get all the CSS text into the data in the styleSheets structure, and the structure is easy to query and modify, which will provide the basis for the later style operations. Converting and normalizing property values in a stylesheet Calculate the specific style of each node in the DOM tree - Now that the style properties have been standardized, the next step is calculating each node’s style properties in the DOM tree, which involves CSS inheritance rules and cascading rules. The first is CSS inheritance, which means that each DOM node contains the style of its parent node. This may be a bit abstract, so let’s combine it with a concrete example of how a style sheet like the one below is applied to a DOM node. Calculating the geometric positions of the visible elements in the DOM treeLayering &amp;&amp; drawingNow that we have the layout tree and the specific location information for each element is calculated, is the next step to start drawing the page? The answer is still no. Because there are many complex effects on the page, such as some complex 3D transformations, page scrolling, or z-indexing to do z-axis sorting, etc. To better to achieve these effects, the rendering engine also needs to generate special layers for specific nodes and a corresponding LayerTree.The rendering engine gives the page several layers stacked together in a specific order to form the final page. After the layer tree is built, the rendering engine draws each layer in the layer tree.Rasterization &amp;&amp; compositingWhen the drawing list of layers is ready, the main thread will submit the drawing list to the compositing thread. Then we have to see what the viewport is. Ssually a page can be large, but the user can only see a part of it. A viewpoint is the part that user can see.In some cases, some layers can be very large. For example, some pages take a long time to scroll to the bottom using the scrollbar, but through the viewport, the user can only see a small part of the page, so in this case, it would be too much overhead to draw all the layers, and it is not necessary. For this reason, the composition thread divides the layers into blocks (tiles), and the composition thread prioritizes the generation of bitmaps according to the blocks near the viewport, and the actual generation of bitmaps is performed by rasterization. Rasterization is the conversion of blocks to bitmaps.The block is the smallest unit of rasterization execution. The rendering process maintains a rasterization thread pool. All block rasterization is performed within the thread pool. If the rasterization operation uses the GPU, the final bitmap generation is done in the GPU, which involves cross-process operations. The rendering process sends the instructions for generating the block to the GPU, and then the bitmap of the generated block is executed in the GPU and saved in the GPU’s memory.ChromeBefore Chrome came out, all browsers on the market were single-process. (I think)Single process browserA single-process browser is one in which all the functional modules of the browser run in the same process, including the network, plug-ins, JavaScript runtime environment, rendering engine, pages, and so on. Such architecture design has a lot of drawbacks. Unstable Early browsers needed plug-ins to implement various powerful features such as Web video, Web games, etc. However, plug-ins were the most problematic modules and also ran in the browser process, so an unexpected plug-in crash could cause the entire browser to crash. In addition to plugins, the rendering engine module is also unstable, and often some complex JavaScript code may cause the rendering engine module to crash. As with plugins, a rendering engine crash can also cause the entire browser to crash. Not smooth By single-process design, the rendering module, JavaScript execution environment, and plug-ins of all pages run in the same thread, which means that only one module can be executed at the same time. Consider an infinite loop script running on that single thread, because this script is infinite loop, when it executes, it monopolizes the entire thread, which causes other modules running in that thread to not have a chance to be executed. Since all the pages in the browser are running in that thread, none of those pages have a chance to execute the task, which causes the whole browser to become unresponsive and laggy. In addition to the above script or plug-in will make the single-process browser lagging, the page memory leak is also a major cause of the single-process slowdown. Usually the kernel of the browser is very complex, running a more complex page and then close the page, there will be memory can not be fully recycled, which leads to the problem that the longer you use, the higher the memory occupation, the slower the browser will become. Insecurity When you run a plug-in on a page, it means that the plug-in can fully operate your computer. If it is a malicious plugin, then it can release viruses, steal your account passwords and cause security problems. As for the page script, it can obtain system privileges through the browser’s vulnerability. These scripts can also do something malicious to your computer after obtaining system privileges, which can also cause security problems. Multi-process browserThe following diagram shows the process architecture of Chrome when it was released in 2008.As you can see from the diagram, Chrome pages run in a separate rendering process. At the same time, the plug-ins in the pages also run in a separate plug-in process, and the processes communicate with each other through the IPC mechanism (as shown in the dashed part of the diagram). So how does Chrome solves the above problems? Since the processes are isolated from each other, a page or plug-in crashes only affects the current page or plug-in process. It does not affect the browser and other pages, which perfectly solves the problem that a page or plug-in crash can cause the whole browser to crash, that is, the problem of instability. Now again, JavaScript runs in the rendering process, so even if JavaScript blocks the rendering process, it only affects the rendered page, not the browser or other pages, because the scripts on other pages run in their rendering process. So when we run the above dead-loop script in Chrome again, only the current page doesn’t respond. The solution to the memory leak is even simpler, because when a page is closed, the entire rendering process is also closed, and the memory occupied by that process is then reclaimed by the system, which easily solves the memory leak problem for browser pages. Lastly, the benefit of using a multi-process architecture is the ability to use a security sandbox, which you can think of as the operating system putting a lock on the process, so that programs inside the sandbox can run but cannot write any data to your hard drive or read any data in sensitive locations, such as your documents and desktop. Chrome locks the plug-in and rendering processes in a sandbox, so that even if a malicious program is executed in the rendering or plug-in process, the program cannot break out of the sandbox and gain access to the system.In fact, the latest Chrome browser includes: The main Browser process. A GPU process. A NetWork process. Multiple rendering processes. Multiple plug-in processes. Let’s break down the functions of each of these processes. Browser process: Mainly responsible for interface display, user interaction, sub-process management, and also provides storage and other functions. Rendering process: The core task is to convert HTML, CSS, and JavaScript into web pages that users can interact with, and both the layout engine Blink and the JavaScript engine V8 run in this process. For security reasons, the rendering processes are run in sandbox mode. When Chrome was first released, there were no GPU processes. The GPU was originally intended for 3D CSS effects, but web pages and Chrome’s UI interface chose to draw with the GPU, making it a common browser requirement. Finally, Chrome also introduced GPU processes to its multi-process architecture. The web process: It was previously run as a module within the browser process, but only recently has it been separated into a separate process. Plug-in process: It is mainly responsible for the running of plug-ins. Since plug-ins are prone to crash, they need to be isolated by the plug-in process to ensure that the crash will not affect the browser and the page.To sum up, opening a page requires at least one network process, one browser process, one GPU process and one rendering process, 4 in total.Although the multi-process model improves browser stability, fluency and security, it also inevitably brings some problems: higher resource usage. Because each process contains a copy of the shared infrastructure (such as the JavaScript runtime environment), the browser consumes more memory resources. More complex architecture. Problems such as high coupling between browser modules and poor scalability can cause the current architecture to be challenging to adapt to new needs." }, { "title": "From source code to runtime", "url": "/posts/from-source-code-to-runtime/", "categories": "System", "tags": "compiler, linker", "date": "2022-04-19 23:14:00 -0400", "snippet": "What happens behind the scene when we compile the following code:#include &lt;iostream&gt;#include \"demo.h\"int main() { std::cout &lt;&lt; \"Demo\" &lt;&lt; std::endl; return GREETING;}// demo.h#define GREETING 0PreprocessingBefore compilation begins, the preprocessor processes macro definitions in the source file to expand or replace the source code. For example, for #include &lt;iostream&gt;, the preprocessor copies the iostream file directly into the source file. However, there is no iostream file in our source code directory, so how does the preprocessor know where to look for it?The preprocessor doesn’t know where all our header files are, and we need to provide the Include Path manually. In the case of g++, this is the -I option. The preprocessor will search through the paths we provide until it finds the header file. If we want to specify multiple lookup directories, we need to provide multiple -I options, and the preprocessor will look them up in order. For example, g++ -Iinclude1 -Iinclude2. For common system headers like iostream, it would be very tedious to specify the search directory manually, so g++ already has many common include paths built-in. By default, the include path will include the source files’ directory.The output of this step is the source code file after macro expansion. Not all languages have a preprocessing step like C/C++, like text replacement.CompilingOnce the source file has been preprocessed, the compiler can be called to compile the source file. In simple terms, each .cpp file can be referred to as a compilation unit. Each compilation unit can be compiled independently. The compiler’s job is to parse the source file, go through a series of complex operations such as lexical analysis, syntax analysis, intermediate code generation, code optimization, etc., and finally generate an assembly file for the target platform.The output is an assembly file, and the assembly language used varies from platform to platform. The most common one is the x86-64 platform.AssemblingOnce we have the assembly file, we can use the assembler to assemble the assembly file into an object file (Object File). The assembler’s job is much simpler. It only needs to translate the instructions in the assembly file, output the corresponding binary machine instructions, and assemble them into the operating system’s object file format.The output of this step is the binary object file (ELF file) for the corresponding platform, which contains the function symbols and the corresponding binary machine code. We can use the readelf command to view the file information of an ELF file.$ readelf -a demo.oELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: REL (Relocatable file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x0 ...Section Headers: [Nr] Name Type Address Offset Size EntSize Flags Link Info Align [ 0] NULL 0000000000000000 00000000 0000000000000000 0000000000000000 0 0 0 [ 1] .text PROGBITS 0000000000000000 00000040 0000000000000091 0000000000000000 AX 0 0 1 [ 2] .rela.text RELA 0000000000000000 00000548 0000000000000108 0000000000000018 I 12 1 8 [ 3] .data PROGBITS 0000000000000000 000000d1 ...LinkingAfter getting the target file, we cannot execute the file yet. This is because target files often rely on external function implementations that are not themselves available. If we use the nm command to look at our target file, we will find many U’s, meaning that these symbols cannot be defined in the current target file. Also, functions need to interact with the OS to execute the program, such as allocating memory, inputting and outputting, and so on. Our target file does not have all these. here is one last step to get the program running in the target file: linking.Linking merges many target files and outputs an executable file.$ ld -static demo.o \\ /usr/lib/x86_64-linux-gnu/crt1.o \\ /usr/lib/x86_64-linux-gnu/crti.o \\ /usr/lib/gcc/x86_64-linux-gnu/9/crtbeginT.o \\ -L/usr/lib/gcc/x86_64-linux-gnu/9 \\ -L/usr/lib/x86_64-linux-gnu \\ -L/lib/x86_64-linux-gnu \\ -L/lib \\ -L/usr/lib \\ -lstdc++ \\ -lm \\ --start-group \\ -lgcc \\ -lgcc_eh \\ -lc \\ --end-group \\ /usr/lib/gcc/x86_64-linux-gnu/9/crtend.o \\ /usr/lib/x86_64-linux-gnu/crtn.o$ lsa.out demo.cpp demo.o$ ./a.outDemoTo call the ld command, pass in the target files we wish to link. Some system libraries are not stored as .o target files, but as .a files. An .a file is really just a bunch of .o files put together. The .a files are often named lib***.a.The linker, like the preprocessor, doesn’t know where all the .a files are, so we need to specify the Library Path with the -L option. Together with the -l above, the linker will find the .a file and link it.A question is how does the linker know where the program should start executing from? Actually the main function is not the real entry point of the program. By default, ld looks for the _start symbol, which is the program’s starting point. This symbol is included in the system runtime library. The _start function in the library will eventually call our main function after initializing the runtime environment.RuntimeLoading a program is similar on different operating systems, which generally requires the following steps: Calling the system function that creates the process System validation parameters to open the specified program file Parsing the program binary format Performing the necessary initialization based on the information provided by the program Create a structure in the kernel that holds the process information Create the virtual memory map, run stack, etc. needed to run the program Jump to the entry point specified in the program file and start running the program The fourth step is more complex and may vary from program to program depending on the operating system. For example, if a program is dynamically linked, then the operating system is responsible for resolving the dynamic linking issue. For example, if we are running a program on Windows, Windows will handle it differently.On Linux, the system call that loads a program into memory is execve, which reads our executable file, loads the code and data into memory, and then starts running our program from the entry address specified in the file. If all our programs were statically linked, the process would be as simple. However, if our program needs to use dynamic libraries, things are not so simple. When Linux detects that our program needs dynamic libraries, it will call the ld.so loader to load our program." }, { "title": "Floating point numbers and fixed point numbers", "url": "/posts/floating-point-number-and-fixed-point-number/", "categories": "Algorithm & Data Structure", "tags": "number format", "date": "2022-04-05 18:12:00 -0400", "snippet": "How can we represent numbers with a point in computer?Fixed point numbersWe know that we can represent 32^2 different numbers with 32 bits, which gives us almost 4 billion distinct numbers. An intuitive idea for representing numbers with a point is to fix the point position. Namely, we use 4 bits to represent integers in decimal (0 to 9), so 32 bits would represent 8 such integers. Then we take the rightmost 2 integers as the fractional part, and fix the point right at that position. In this way, we can represent 100 million real numbers from 0 to 999999.99 in 32 bits.Such binary representation of point decimal is called BCD (Binary-Coded Decimal). It is most often used in supermarkets and banks where we have decimals up to the cent position.However, the drawback is clear: Waste of bits. With 32 bits we could represent 4 billion different numbers theoretically, but we can only represent 100 million numbers in BCD There is no way to represent huge numbers and tiny numbers simultaneously in this way. We can’t let our computers deal only a small fixed range of numbers.Floating point numbersV = (-1)^s * M * (2)^Ewhere (-1)^s denotes the sign bit, when s=0, V is positive; when s=1, V is negative. M indicates a valid number, greater than or equal to 1, less than 2. 2^E denotes the exponent bit.IEEE 754 specifies that for 32-bit floating-point numbers, the highest 1 bit is the sign bit s, followed by the next 8 bits are the exponent E, and the remaining 23 bits are the significant digits M.For 64-bit floating-point numbers, the highest 1 bit is the sign bit S, followed by the next 11 bits are the exponent E, and the remaining 52 bits are the significant digits M.Note that IEEE 754 has some special rules for the valid number M and exponent E: 1 ≤ M &lt; 2, that is, M can be written in 1.xxxxxxx, where xxxxxxx indicates the fractional part. IEEE 754 stipulates that when M is saved internally in a computer, the first bit of this number is always 1 by default, so it can be rounded off and only the xxxxxxx part after it is saved. For example, when saving 1.01, only 01 is saved, and when it is read, the first 1 is added. The purpose of doing this is to save 1 valid digit. Take 32-bit floating-point number as an example, only 23 bits are left for M. After rounding off the first 1, it is equal to save 24 valid digits. E is an unsigned integer. However, we know that E in scientific notation can be negative, so IEEE 754 stipulates that the true value of E must be subtracted by an intermediate number, which is 127 for 8-bit E and 1023 for 11-bit E. E is not all 0 or not all 1. In this case, the floating point number is expressed using the above rule, i.e., the calculated value of the exponent E is subtracted from 127 (or 1023) to get the true value, and then the valid number M is preceded by the first 1. E is all 0. In this case, the exponent E of the floating point number is equal to 1-127 (or 1-1023), and M is no longer preceded by the first 1, but reduced to a decimal of 0.xxxxxxx. This is done to represent ±0, and tiny little numbers close to 0. E is all 1. In this case, if M is all 0, it means ± infinity (positive or negative depending on the sign digit s); if M is not all 0, it means the number is not a number (NaN). Bfloat16The bfloat16 format was developed by Google Brain, an artificial intelligence research group at Google. It is a truncated (16-bit) version of the 32-bit IEEE 754 single-precision floating-point format (binary32) with the intent of accelerating machine learning and near-sensor computing.The main difference is that bfloat16 supports only an 8-bit significand rather than the 24-bit significand of the binary32 format.ReferenceWiki: Computer number formatWiki: bfloat16" }, { "title": "Select, poll and epoll", "url": "/posts/select-poll-and-epoll/", "categories": "System", "tags": "network, server", "date": "2022-03-17 11:23:00 -0400", "snippet": "What is I/O multiplexingConsider a program that reads from stdin and sends the read in content to a socket channel and vice versa (reads from socket channel and writes to stdout). We could use the fgets method to wait for the stdin. But once we do that, there is no way to read the data when the socket has data. We could also use the read method to wait for the socket to return with data. but once we do that, there is also no way to read in the data and send it to the other side when the stdin has data.Developers propose I/O multiplexing to solve such scenarios. We can consider stdin as one way of I/O and sockets as another way of I/O. Multiplexing means that if there is an “event” in any way of I/O, the application is notified to handle the corresponding I/O event, so that our program becomes a multi-tasker as if it can handle multiple I/O events at the same time.The select and poll models are two such I/O multiplexing implementations.SelectHere is the select function signature:int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);The select function is used to detect an “event” on one or more of the sockets in a group, where “events” are generally divided into three categories. Readable events, which generally mean that the recv or read function can be called to read data from the socket Writable event, which generally means that the send or write function can be called to “send out” the data. Exception events, where a socket has an exception.The parameters are: nfds, also known as fd for Linux sockets, is set to the maximum fd value of all fd’s that need to be listened to using the select function plus 1 (0-based index). readfds, the set of fd’s that need to listen for readable events. writefds, the set of fd’s to listen to for writable events. exceptfds, the set of fd’s to listen to for exception events. We can utilize the following macros:void FD_ZERO(fd_set *fdset); // set all elements of fdset to 0void FD_SET(int fd, fd_set *fdset); // set the element corresponding to the socketvoid FD_CLR(int fd, fd_set *fdset); // clear the element corresponding to the socketint FD_ISSET(int fd, fd_set *fdset); // determines whether the element corresponding to the socket is 0 or 1Where 0 means no processing is required and 1 means the opposite.However, the select function has two design flaws: the select function limits the number of file descriptors a single process can listen to, which is determined by __FD_SETSIZE, with a default value of 1024. when the select function returns, we need to iterate through the descriptor to find out which descriptors are ready. This traversal process generates some overhead, which can slow down the program’s performance.As a result, the poll function has been proposed to solve the shortage of 1024 file descriptors to which the select function is limited.PollLet’s first take a look at the poll function definition:int poll (struct pollfd *__fds, nfds_t __nfds, int __timeout); Where the argument *__fds is the pollfd structure array, the argument __nfds represents the number of elements of the *__fds array, and __timeout represents the timeout for the poll function to block.pollfd contains three member variables, fd, events, and revents, indicating the file descriptor to listen on, the type of event to listen on, and the type of event that actually occurred.struct pollfd { int fd; // the file descriptor to listen on short int events short int events; //the type of event to listen to short int revents short int revents; //the type of event that actually occurs};The poll process can be divided into three steps: to create a pollfd array and a listening socket and bind them; to add the listening socket to the pollfd array and set it to listen for read events, that is, connection requests from the client; to call the poll function in a loop to detect if there are ready file descriptors in the pollfd array.The main improvement of the poll function over the select function is that it allows more than 1024 file descriptors to be listened to at once. However, after calling the poll function, we still need to iterate through each file descriptor, check if it is ready, and then process it.EpollThe epoll mechanism avoids traversing every descriptor. It uses the epoll_event structure to record the file descriptor to be listened to and the type of event it is listening for, similar to the pollfd structure used in the poll mechanism. The epoll_event structure contains the epoll_data_t union variable and the events variable, which is an integer. epoll_data_t has the member variable fd that records the file descriptor. The events variable takes a different macro value to represent read, write &amp; error events.typedef union epoll_data{ ... int fd; //record file descriptor ...} epoll_data_t;struct epoll_event{ uint32_t events; //events that epoll listens to epoll_data_t data; //application data};When using select or poll functions, after creating a collection of file descriptors or pollfd array, we can add the file descriptors we need to listen to to the array. But for the epoll mechanism, we need to call the epoll_create function first for an epoll instance. This epoll instance maintains two structures, one for the file descriptors to listen to and one for the file descriptors ready to be returned to the user program for processing. So, when we use the epoll mechanism, we don’t have to iterate through which file descriptors are ready, as we do with select and poll. This makes epoll much more efficient than select and poll.kqueueLong before Linux implemented epoll, Windows introduced IOCP, an asynchronous I/O model to support highly concurrent network I/O, in 1994, and the famous FreeBSD introduced kqueue, an I/O event distribution framework, in 2000. Linux introduced epoll in 2002, although related work was discussed and designed as early as 2000.Why didn’t Linux port FreeBSD’s kqueue directly over to epoll instead? Let’s look at the usage of kqueue. kqueue also needs to create an object called kqueue first, then, through this object, call the kevent function to add the event of interest, and through this kevent function, wait for the event to happen.int kqueue(void);int kevent(int kq, const struct kevent *changelist, int nchanges, struct kevent *eventlist, int nevents, const struct timespec *timeout);void EV_SET(struct kevent *kev, uintptr_t ident, short filter, u_short flags, u_int fflags, intptr_t data, void *udata);struct kevent { uintptr_t ident; /* identifier (e.g., file descriptor) */ short filter; /* filter type (e.g., EVFILT_READ) */ u_short flags; /* action flags (e.g., EV_ADD) */ u_int fflags; /* filter-specific flags */ intptr_t data; /* filter-specific data */ void *udata; /* opaque user data */};In his original vision, Linus stated that he thought that arrays like select or poll were OK, while queues were bad. Quoted below: So sticky arrays of events are good, while queues are bad. Let’s take that as one of the fundamentals." }, { "title": "How to shorten the url", "url": "/posts/how-to-shorten-the-url/", "categories": "Algorithm & Data Structure", "tags": "hashmap", "date": "2022-03-01 20:23:00 -0500", "snippet": "Have you ever used the short URL service? If we post a message with a URL in a microblog, the microblog will convert the URL inside into a shorter URL. If we visit this short URL, it is the same as visiting the original URL.The short URL service is straightforward: it converts a long URL into a short one. When the user clicks on the short URL, the short URL service will redirect the browser to the original URL. How does this process work? The browser will first visit the short URL service, get the original URL through the short URL, and then visit the page through the original URL.How to generate short URLs via hashing?Hashing algorithm can transform a string, no matter how long, into a fixed hash value length. We can use the hash algorithm to generate short URLs. We have already mentioned some hashing algorithms, such as MD5, SHA, etc. However, we do not need these complex hashing algorithms. In the problem of generating short URLs, after all, we do not need to consider the difficulty of reverse decryption, so we only need to care about the computational speed and conflict probability of the hash algorithm. Many hashing algorithms can meet such requirements, and one of the more famous and widely used hashing algorithms is the MurmurHash algorithm. Although this hashing algorithm was only invented in 2008, it is now widely used in Redis, MemCache, Cassandra, HBase, Lucene and much other famous software. To generate the short URLs as short as possible, we can choose the 32bits hash. For url https://jiahuanglin.xyz/posts/how-to-design-a-key-value-storage-system/, the MurmurHash hash is 2509295521, and the short URL can just be https://tiny.com/2509295521However, as you may have seen, the short URL obtained by MurmurHash is still long and seems to be in a different format than the one we started with. But don’t worry, we can easily make the short URLs shorter by changing the representation of the hash value just a little. We can convert the hash value of 10 into a higher binary hash value, so the hash value becomes shorter. As we know, in hexadecimal, we use A to F to represent 10 to 15. In the URL, 62 characters like 0 to 9, a to z, A to Z, commonly used as legal characters. To make the hash value as short as possible, we can convert the hash value from 10 to 62. I’ve written the details of the calculation process here. The final short URL expressed in 62 notation is http://t.cn/cgSqq.Hash conflictsHow to solve the hash conflict problem? However, as we mentioned earlier, one of the problems that hash algorithms cannot avoid is hash conflicts. Once the conflict occurs, it will result in two original URLs being transformed into the same short URL. When the user visits the short URL, we cannot determine which original URL the user wants to visit. So how to solve this problem? In general, we store the correspondence between the short URL and the original URL so that subsequent users can find the original URL according to the correspondence when they visit the short URL. There are many ways to store this correspondence, such as designing our storage system or using an off-the-shelf database.Let’s take MySQL as an example. Suppose that the correspondence between the short URL and the original URL is stored in the MySQL database. When a new original URL needs to be generated as a short URL, we first use MurmurHash algorithm to generate the short URL. Then, we take the newly generated short URL and look it up in the MySQL database. If no identical short URL is found, the newly generated short URL has no conflict. Then we return the short URL to the user (the user who requested the short URL) and store the correspondence between the short URL and the original URL in the MySQL database. If we find the same short URL in the database, it does not necessarily mean that there is a conflict. If the original URL in the database is the same as the original URL we are working on now, it means that someone has already requested the short URL of the original URL. We can then take the short URL and use it directly. If the original URL recorded in the database is not the same as the original URL we are processing, there is a conflict in the hash algorithm. Different original URLs, after calculation, get duplicate short URLs. We So what should we do if there’s a conflict? We can append a string of special characters to the original URL, such as “[DUPLICATED]”, and then recalculate the hash value, the probability that both hash calculations conflict is obviously very low. Suppose there is a very extreme case, and there is a conflict, we can append another string, such as “[OHMYGOD]”, and then calculate the hash value. Then the calculated hash value is stored in the MySQL database together with the text of the original URL after appending the special string. When the user accesses the short URL, the short URL service first looks up the corresponding original URL in the database through the short URL. Suppose the original URL has appended special characters (which can be easily found by string matching algorithm). In that case, we will remove the special characters first, and then return the original URL without special characters to the browser.Performance optimization for database queriesTo determine whether the generated short URLs are conflicting, we need to take the generated short URLs and look them up in the database. If there is a lot of data stored in the database, the search will be very slow, which will affect the performance of the short URL service. So is there any means of optimization?Yes, we can add a B+ tree index to the short URL field. This way, the speed of querying the original URL through the short URL increases significantly. In real software development, there is a trick we can use to increase the speed even further. In the process of short URL generation, we will deal with the database twice, that is, we will execute two SQL statements. The first SQL statement queries the correspondence between the short URL and the original URL through the short URL The second SQL statement stores the correspondence between the newly generated short URL and the original URL into the database.Generally, the database and the application service are deployed on two separate servers or virtual servers. Then the execution of two SQL statements requires two network communications. This IO communication time consuming and SQL statement execution is the performance bottleneck of the short URL service. Therefore, we need to minimize the number of SQL statements to improve performance.How can we reduce the SQL statements? We can add a unique index to the short URL field in the database (not only the index, but also the requirement that there should be no duplicate data in the table). When there is a new original URL to generate a short URL, we do not firstly take the generated short URL and check for the duplication in the database, instead we directly store the generated short URL and the corresponding original URL into the database. If the database can write the data normally, it means that there is no violation of the unique index, that is, this new generated short URL does not conflict. Of course, if the database gives us an exception about the violation of the unique index, then we have to execute the query and write process again, and the number of SQL statements will increase instead of decrease. However, in most cases, when we insert the newly generated short URL and the corresponding original URL into the database, there is no conflict. So, in most cases, we only need to execute one written SQL statement. So, overall, the total number of SQL statements executed will be greatly reduced.There is another way to optimize the number of SQL statements with the help of Bloom filters. We take the short URLs we have generated and build them into Bloom filters. We take the short URLs that have been generated and build them into Bloom filters. We know that Bloom filters are a relatively memory-efficient storage structure, and a Bloom filter of 1 billion in length requires only about 125MB of memory space. When a new short URL is generated, we first take the newly generated short URL and look it up in the Bloom filter. If the search result is no, then the newly generated short URL is not conflicting. At this time, we need to execute the SQL statement that writes the short URL and the corresponding original web page again. Therefore, the total number of SQL statements executed is reduced by querying the Bloom filter first.How to generate short URLs via ID generator?We can maintain an ID self-incrementing generator. When the short URL service receives a request to convert a raw URL into a short URL, it first takes a number from the ID generator, converts it into a 62-entry representation, and splices it into the domain name of the short URL service (e.g., http://t.cn/) to form the final short URL. Finally, we still store the generated short and original URLs in the database. The theory is very simple to understand.However, there are 2 problems to deal with here: The same original URL may correspond to a different short URL. Every time a new original URL comes, we generate a new short URL. This practice will lead to two identical original URL generating a different short URL. How should this be handled? We have two processing ideas. Not to deal with it. It sounds nonsensical, but I’ll explain. The same original URL corresponds to a different short URL, which is acceptable to users. In most short URL application scenarios, the user only cares whether the short URL can correctly jump to the original URL. As for what the short URL looks like, he does not care. Therefore, even if the original URL is the same, the short URL generated twice is not the same, which does not affect the user’s use. Processing idea of generating short URLs with the help of hashing algorithm, when we want to generate a short URL to the original URL, we have to take the original URL in the database to see if the same original URL already exists in the database. If the database exists, we will take out the corresponding short URL and return it to the user directly. However, there is a problem with this processing idea, and we need to add indexes to both the short URL and the original URL fields in the database. The index on the short URL is to improve the user’s speed to query the short URL corresponding to the original web page, and the index on the original URL is to speed up the short URL query through the original URL just mentioned. Although this solution can meet the demand of “the same original URL corresponds to the same short URL”, there is a cost: on the one hand, the two indexes will occupy more storage space, and on the other hand, the indexes will lead to the degradation of the performance of insertion and deletion operations. Performance issues. There are many ways to implement ID generators, such as database self-incrementing fields. Of course, we can also maintain a counter and keep adding one and one. However, a counter to cope with frequent short URL generation requests is obviously a bit overwhelming since the counter must ensure that the generated IDs are not duplicated, which generally means that locks are required. So how can the performance of the ID generator be improved? We can load the ID generator with multiple predecessors. The ID generator will send ID numbers to each of the predecessors in bulk. When we receive a request for short URL generation, we select a predecessors to fetch the number. This significantly increases the concurrent numbering capability by having multiple front senders. Instead of using an architecture with one ID generator and multiple predecessors, we directly implement multiple ID generators serving simultaneously. To ensure that the IDs generated by each ID generator are not duplicated. We require each ID generator to follow certain rules to generate ID numbers. For example, the first ID generator can only generate IDs with the last number 0, the second ID generator can only generate IDs with the last number 1, and so on. This also improves the efficiency of ID generation by having multiple ID generators working simultaneously. " }, { "title": "Visitor design pattern and k8s kubectl implementation", "url": "/posts/visitor-design-pattern-and-k8s-kubectl-implementation/", "categories": "Software", "tags": "design pattern, k8s, Java, Go", "date": "2022-02-11 21:11:00 -0500", "snippet": "Visitor patternA machine learning framework will typically implement its dataset I/O functionality. Data source are usually of different types and format. Say we have 3 kinds of format to parse, namely CSV, text and images. We could have our code implemented like the following:public abstract class Parser { protected String filepath; public Parser(String filepath) { this.filepath = filepath; } public abstract Dataset parse();}public class CsvParser extends Parser { @Override public Dataset parse() { //... }}public class TextParser extends Parser { @Override public Dataset parse() { //... }}public class ImageParser extends Parser { @Override public Dataset parse() { //... }}The disadvantage of this approach is that data sources can be in many different formats, and the framework needs to provide additional I/O support for each data source. We already have 3 classes with 3 different formats and one parsing functionality. If the framework also needs to provide serialization, compression, and transformation for each output data format, then even with only three data formats, we would have 3 * 4 = 12 classes. Given that the number of data source format can be big, the above implementation doesn’t look like a scalable solution.If we could decouple the data format from the I/O function, then the total number of classes would be an addition result rather than a multiplication result. The implementation looks like the following:public abstract class DataFile { protected String filepath; public DataFile(String filepath) { this.filepath = filepath; }}public class CsvFile extends DataFile { public CsvFile(String filepath) { super(filepath); }}public class TextFile extends DataFile { public TextFile(String filepath) { super(filepath); }}public class ImageFile extends DataFile { public ImageFile(String filepath) { super(filepath); }}public class DataParser { // return a parsed dataset object public Dataset parse(CsvFile file) { // ... } public Dataset parse(TextFile file) { // ... } public Dataset parse(ImageFile file) { // ... }}public class DataTransformer { public Dataset transform(CsvFile file) { // ... } public Dataset transform(TextFile file) { // ... } public Dataset transform(ImageFile file) { // ... }}However, the above code doesn’t compile in Java. The implementation makes an inheritance relationship with the DataFile class and then uses overloaded functions in the Parser/Serializer class to handle different file types. But because the binding time of Java polymorphism and function overloading is different, the compiler doesn’t know what the parameters passed in the end during compile time, so it reports an error.We can apply visitor pattern like the following:public abstract class DataFile { protected String filepath; public DataFile(String filepath) { this.filepath = filepath; } public abstract void accept(Visitor visitor);}public class CsvFile extends DataFile { public CsvFile(String filepath) { super(filepath); } @Override public void accept(Visitor visitor) { visitor.visit(this); }}public class TextFile extends DataFile { public TextFile(String filepath) { super(filepath); } @Override public void accept(Visitor visitor) { visitor.visit(this); } // ...}public class ImageFile extends DataFile { public ImageFile(String filepath) { super(filepath); } @Override public void accept(Visitor visitor) { visitor.visit(this); } // ...}public interface Visitor { public Dataset visit(CsvFile file); public Dataset visit(TextFile file); public Dataset visit(ImageFile file);}public class DataParser implements Visitor { // return a parsed dataset object public Dataset visit(CsvFile file) { // ... parsing } public Dataset visit(TextFile file) { // ... parsing } public Dataset visit(ImageFile file) { // ... parsing }}public class DataTransformer { // return a parsed dataset object public Dataset visit(CsvFile file) { // ... transform } public Dataset visit(TextFile file) { // ... transform } public Dataset visit(ImageFile file) { // ... transform }}Visitor pattern in Kubectltype VisitorFunc func(*Info, error) errortype Visitor interface { Visit(VisitorFunc) error}type Info struct { Namespace string Name string OtherThings string}func (info *Info) Visit(fn VisitorFunc) error { return fn(info, nil)}Note that the Visit() method of the Visitor interface is implemented for Info, and instead of passing the object to Visit(), the implementation directly passes the funtion pointer (another form of runtime overloading).// visitors for each field of Info struct// Name visitortype NameVisitor struct { visitor Visitor}func (v NameVisitor) Visit(fn VisitorFunc) error { return v.visitor.Visit( func(info *Info, err error) error { fmt.Println(\"NameVisitor() before call function\") err = fn(info, err) if err == nil { fmt.Printf(\"==&gt; Name=%s, NameSpace=%s\\n\", info.Name, info.Namespace) } fmt.Println(\"NameVisitor() after call function\") return err } )}// Other visitortype OtherThingsVisitor struct { visitor Visitor}func (v OtherThingsVisitor) Visit(fn VisitorFunc) error { return v.visitor.Visit( func(info *Info, err error) error { fmt.Println(\"OtherThingsVisitor() before call function\") err = fn(info, err) if err == nil { fmt.Printf(\"==&gt; OtherThings=%s\\n\", info.OtherThings) } fmt.Println(\"OtherThingsVisitor() after call function\") return err } )}// Log visitortype LogVisitor struct { visitor Visitor}func (v LogVisitor) Visit(fn VisitorFunc) error { return v.visitor.Visit( func(info *Info, err error) error { fmt.Println(\"LogVisitor() before call function\") err = fn(info, err) fmt.Println(\"LogVisitor() after call function\") return err } )}ReferenceKubectl source code visitor.go" }, { "title": "Writing a LLVM pass", "url": "/posts/writing-a-llvm-pass/", "categories": "System", "tags": "compiler, LLVM", "date": "2022-01-17 09:01:00 -0500", "snippet": "Basic function passWe can register our own function pass for LLVM to execute for us. The following are quoted from the LLVM documentation: All FunctionPass execute on each function in the program independent of all of the other functions in the program. FunctionPasses do not require that they are executed in a particular order, and FunctionPasses do not modify external functions.#include \"llvm/ADT/Statistic.h\"#include \"llvm/Pass.h\"#include \"llvm/IR/Function.h\"#include \"llvm/Support/raw_ostream.h\"using namespace llvm;STATISTIC(MyCounter, \"Counts number of functions greeted\");namespace { struct MyPass : public FunctionPass { static char ID; MyPass() : FunctionPass(ID) {} virtual bool runOnFunction(Function &amp;F) { ++MyCounter; errs() &lt;&lt; \"A function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\"; return false; } };}char MyPass::ID = 0;// Register the pass so `opt -skeleton` runs it.static RegisterPass&lt;MyPass&gt; X(\"mypass\", \"a useless pass\");Vectorization PassSimilar to function pass, we can register a customized loop pass which does all the vectorization work of a loop and then LLVM can execute the pass for us. The following are quoted from LLVM documentation: All LoopPass execute on each loop in the function independent of all of the other loops in the function. LoopPass processes loops in loop nest order such that outer most loop is processed last. LoopPass subclasses are allowed to update loop nest using LPPassManager interface. Implementing a loop pass is usually straightforward. LoopPasses may overload three virtual methods to do their work. All these methods should return true if they modified the program, or false if they didn’t. A LoopPass subclass which is intended to run as part of the main loop pass pipeline needs to preserve all of the same function analyses that the other loop passes in its pipeline require. To make that easier, a getLoopAnalysisUsage function is provided by LoopUtils.h. It can be called within the subclass’s getAnalysisUsage override to get consistent and correct behavior. Analogously, INITIALIZE_PASS_DEPENDENCY(LoopPass) will initialize this set of function analyses.Types involved Value: Note that Value is the base class of all values computed by a program that may be used as operands to other values. Value is the super class of other important classes such as Instruction and Function. All Values have a Type. Type is not a subclass of Value. Some values can have a name and they belong to some Module. Setting the name on the Value automatically updates the module’s symbol table. Every value has a “use list” that keeps track of which other Values are using this Value. A Value can also have an arbitrary number of ValueHandle objects that watch it and listen to RAUW and Destroy events. See llvm/IR/ValueHandle.h for details. IRBuilder PHINode Loop LPPassManager#include \"llvm/Analysis/LoopInfo.h\"#include \"llvm/Analysis/LoopPass.h\"#include \"llvm/Analysis/ScalarEvolution.h\"#include \"llvm/IR/DerivedTypes.h\"#include \"llvm/IR/Function.h\"#include \"llvm/IR/LegacyPassManager.h\"#include \"llvm/IR/IRBuilder.h\"#include \"llvm/IR/ValueMap.h\"#include \"llvm/Pass.h\"#include \"llvm/Support/raw_ostream.h\"#include \"llvm/Support/ScalableSize.h\"#include \"llvm/Transforms/IPO/PassManagerBuilder.h\"#include \"llvm/Transforms/Scalar/IndVarSimplify.h\"#include \"llvm/Transforms/Utils/BasicBlockUtils.h\"#include \"llvm/Transforms/Utils/LoopUtils.h\"#include \"llvm/Transforms/Utils/Mem2Reg.h\"using namespace llvm;namespace { struct VecotrizationPass : public LoopPass { static char ID; VecotrizationPass() : LoopPass(ID) {} // create a vector of values Value* vectorizeValue(Value* val, int VECTOR_SIZE, PHINode* indVar) { if (auto* constant = dyn_cast&lt;ConstantData&gt;(val)) { return ConstantDataVector::getSplat(VECTOR_SIZE, constant); } else if (auto* inst = dyn_cast&lt;Instruction&gt;(val)) { IRBuilder&lt;&gt; builder(inst); Value* initVec; if (auto* intType = dyn_cast&lt;IntegerType&gt;(inst-&gt;getType())) { initVec = ConstantDataVector::getSplat(VECTOR_SIZE, ConstantInt::get(intType, 0)); } else { initVec = ConstantDataVector::getSplat(VECTOR_SIZE, ConstantFP::get(val-&gt;getType(), 0.0)); } builder.SetInsertPoint(inst-&gt;getNextNode()); Value* curVec = initVec; for (int i = 0; i &lt; VECTOR_SIZE; i++) { curVec = builder.CreateInsertElement(curVec, inst, builder.getInt64(i)); } // vector of inductive variables has to have its stride if (val == indVar) { std::vector&lt;uint64_t&gt; strides; for (uint64_t i = 0; i &lt; VECTOR_SIZE; i++) { strides.push_back(i); } ArrayRef&lt;uint64_t&gt; strideRef(strides); Value* strideVec = ConstantDataVector::get(indVar-&gt;getContext(), strideRef); Value* resultVec = builder.CreateAdd(curVec, strideVec); return resultVec; } else { return curVec; } } return NULL; } virtual bool runOnLoop(Loop* L, LPPassManager &amp;LPM) { int VECTOR_SIZE = 4; LoopInfo &amp;LI = getAnalysis&lt;LoopInfoWrapperPass&gt;().getLoopInfo(); ScalarEvolution &amp;SE = getAnalysis&lt;ScalarEvolutionWrapperPass&gt;().getSE(); // errs() &lt;&lt; \"name: \" &lt;&lt; L-&gt;getName() &lt;&lt; \"\\n\"; PHINode* indVar = L-&gt;getCanonicalInductionVariable(); BinaryOperator* indVarUpdate = NULL; ICmpInst* cmp = NULL; // check loop bound and see if it is divisible by VECTOR_SIZE bool hasVectorizableLoopBound = false; if (BasicBlock* latchBlock = L-&gt;getExitingBlock()) { for (auto&amp; lbInst : *latchBlock) { if (auto* exitingBranch = dyn_cast&lt;BranchInst&gt;(&amp;lbInst)) { // branch must have a condition (which sets the loop bound) if (exitingBranch-&gt;isConditional()) { if (cmp = dyn_cast&lt;ICmpInst&gt;(exitingBranch-&gt;getCondition())) { Value* op1 = cmp-&gt;getOperand(0); Value* op2 = cmp-&gt;getOperand(1); Value* loopBound = op1 == indVar ? op2 : (op2 == indVar ? op1 : NULL); // loop bound must be a constant. otherwise we can't vectorize if (loopBound != NULL) { if (auto* loopBoundConst = dyn_cast&lt;ConstantInt&gt;(loopBound)) { int64_t intBound = loopBoundConst-&gt;getSExtValue(); hasVectorizableLoopBound = intBound % VECTOR_SIZE == 0; } } else { // errs() &lt;&lt; \"no loop bound found!\\n\"; } } } } } } if (!hasVectorizableLoopBound) return false; // find indvar update instruction // dont vectorize unless we find an update instruction bool hasLoopUpdate = false; for (int i = 0; i &lt; indVar-&gt;getNumIncomingValues(); i++) { Value* incomingVal = indVar-&gt;getIncomingValue(i); if (auto* binOp = dyn_cast&lt;BinaryOperator&gt;(incomingVal)) { bool isIndVarOp = binOp-&gt;getOperand(0) == indVar || binOp-&gt;getOperand(1) == indVar; if (isIndVarOp &amp;&amp; indVarUpdate == NULL) { indVarUpdate = binOp; hasLoopUpdate = true; // multiple updates to the indvar is not allowed! } else if (isIndVarOp &amp;&amp; indVarUpdate != NULL) { hasLoopUpdate = false; } } } if (!hasLoopUpdate) return false; // check that all instructions in the body are vectorizable bool hasCrossIterationDependencies = false; std::set&lt;Value*&gt; vectorizedSet; for (auto &amp;B : L-&gt;getBlocks()) { for (auto &amp;I : *B) { if (hasCrossIterationDependencies) break; if (&amp;I == cmp || &amp;I == indVar || &amp;I == indVarUpdate) { // approximate checking for cross-iteration dependencies by // checking if GEPs index through the inductive variable only } else if (auto* gep = dyn_cast&lt;GetElementPtrInst&gt;(&amp;I)) { for (auto&amp; index : gep-&gt;indices()) { if (index != indVar &amp;&amp; !L-&gt;isLoopInvariant(index)) { // errs() &lt;&lt; \"cross gep! index: \" &lt;&lt; *index &lt;&lt; \"\\n\"; hasCrossIterationDependencies = true; } } vectorizedSet.insert(gep); } else if (auto* branch = dyn_cast&lt;BranchInst&gt;(&amp;I)) { if (branch-&gt;isConditional()) { if (L-&gt;isLoopInvariant(branch-&gt;getCondition())) { hasCrossIterationDependencies = true; } } } else { for (int i = 0; i &lt; I.getNumOperands(); i ++) { Value* operand = I.getOperand(i); if (vectorizedSet.count(operand) == 0 &amp;&amp; !L-&gt;isLoopInvariant(operand) &amp;&amp; operand != indVar) { hasCrossIterationDependencies = true; } } vectorizedSet.insert(&amp;I); } } } if (hasCrossIterationDependencies) return false; bool isVectorizable = hasVectorizableLoopBound &amp;&amp; hasLoopUpdate &amp;&amp; !hasCrossIterationDependencies; // errs() &lt;&lt; \"vectorizable? \" &lt;&lt; isVectorizable &lt;&lt; \"\\n\"; // vectorize! if (isVectorizable) { // maintain a map of vectorized instructions // if an instruction reads a vectorized instruction, // it is also vectorized std::map&lt;Value*,Value*&gt; valmap; std::list&lt;Instruction*&gt; removedInstrs; for (auto &amp;B : L-&gt;getBlocks()) { for (auto&amp; I : *B) { if (&amp;I == cmp || &amp;I == indVarUpdate || &amp;I == indVar) { // GEP: array accesses should be vectorized by bitcasting results // of GEPs from t* to &lt;n x t&gt;*, where n is the vector size } else if (auto* gep = dyn_cast&lt;GetElementPtrInst&gt;(&amp;I)) { bool isGEPLoopInvariant = true; for (auto&amp; index : gep-&gt;indices()) { isGEPLoopInvariant = isGEPLoopInvariant &amp;&amp; L-&gt;isLoopInvariant(index); } if (!isGEPLoopInvariant) { IRBuilder&lt;&gt; builder(gep); builder.SetInsertPoint(gep-&gt;getNextNode()); if (auto* elementPtrType = dyn_cast&lt;PointerType&gt;(gep-&gt;getType())) { Type* arrIndType = PointerType::getUnqual( VectorType::get(elementPtrType-&gt;getElementType(), ElementCount(VECTOR_SIZE, false))); Value* arrayIndVec = builder.CreateBitCast(gep, arrIndType); valmap.insert(std::pair&lt;Value*,Value*&gt;(gep,arrayIndVec)); } } // generic branch that checks operands of instructions } else if (dyn_cast&lt;BinaryOperator&gt;(&amp;I) != NULL || dyn_cast&lt;ICmpInst&gt;(&amp;I) != NULL || dyn_cast&lt;LoadInst&gt;(&amp;I) != NULL || dyn_cast&lt;StoreInst&gt;(&amp;I) != NULL) { for (int i = 0; i &lt; I.getNumOperands(); i++) { Value* operand = I.getOperand(i); std::map&lt;Value*,Value*&gt;::iterator it = valmap.find(operand); if (it != valmap.end()) { I.setOperand(i, it-&gt;second); } else { Value* newOperand = vectorizeValue(operand, VECTOR_SIZE, indVar); I.setOperand(i, newOperand); valmap.insert(std::pair&lt;Value*,Value*&gt;(operand, newOperand)); } } Type* retType = I.getType(); if (retType != NULL &amp;&amp; dyn_cast&lt;StoreInst&gt;(&amp;I) == NULL) { Type* retVecType = VectorType::get(retType, ElementCount(VECTOR_SIZE, false)); I.mutateType(retVecType); } valmap.insert(std::pair&lt;Value*,Value*&gt;(&amp;I, &amp;I)); } } } // remove instructions out of the loop (so iterators aren't messed up) for (auto* removedInstr : removedInstrs) { removedInstr-&gt;eraseFromParent(); } // finally, update inductive variable stride to be VECTOR_SIZE if (indVarUpdate-&gt;getOperand(0) == indVar) { indVarUpdate-&gt;setOperand(1, ConstantInt::get(indVar-&gt;getType(), VECTOR_SIZE)); } else { indVarUpdate-&gt;setOperand(0, ConstantInt::get(indVar-&gt;getType(), VECTOR_SIZE)); } } return isVectorizable; } virtual void getAnalysisUsage(AnalysisUsage &amp;AU) const override { getLoopAnalysisUsage(AU); // AU.addRequired&lt;PromotePass&gt;(); } };}char VecotrizationPass::ID = 0;// Register the pass so `opt -vectorization` runs it.static RegisterPass&lt;VecotrizationPass&gt; X(\"vectorization\", \"an auto-vec pass\");ReferenceWriting An LLVM PassLLVM Hello Word PassLLVM DocumentationLLVM Concepts" }, { "title": "My notes of reading effective C++", "url": "/posts/my-notes-of-reading-effective-c++/", "categories": "Software", "tags": "C++", "date": "2022-01-03 02:04:00 -0500", "snippet": " Following is my note of reading effective c++ (this is an on-going post)Copy all parts of an objectIn a mature object-oriented C++ system, there are only two ways of copying objects: copy constructors and assignment operators referred to as copy functions. Copy functions are compiler-generated functions by default, and the default copy function does copy the object in its entirety. Still, sometimes we choose to overload the copy function. And that’s where the problem arises!A correct implementation of the copy function would look like this:class Customer{ string name;public: Customer(const Customer&amp; rhs): name(rhs.name){} Customer&amp; operator=(const Customer&amp; rhs){ name = rhs.name; // copy rhs's data return *this; } };Perfect, right? But then, one day, we add a new data member and forget to update the copy function:class Customer{ string name; Date lastTransaction;public: Customer(const Customer&amp; rhs): name(rhs.name){} Customer&amp; operator=(const Customer&amp; rhs){ name = rhs.name; // copy rhs's data return *this; } };The lastTransaction is ignored and the compiler does not give any warnings (even at the highest warning level). Another common scenario is when we inherit from a parent class:class PriorityCustomer: public Customer {int priority;public: PriorityCustomer(const PriorityCustomer&amp; rhs) : priority(rhs.priority){} PriorityCustomer&amp; operator=(const PriorityCustomer&amp; rhs){ priority = rhs.priority; } };The above code looks fine, but we forgot to copy the part of the parent class:class PriorityCustomer: public Customer {int priority;public: PriorityCustomer(const PriorityCustomer&amp; rhs) : Customer(rhs), priority(rhs.priority){} PriorityCustomer&amp; operator=(const PriorityCustomer&amp; rhs){ Customer::operator=(rhs); priority = rhs.priority; } };In short, when we implement the copy function: make a complete copy of the current object’s data (local data). call all the corresponding copy functions in the parent class.You may notice the repetition of the code, but don’t let the copy constructor and the assignment operator call each other. They have completely different semantics! C++ doesn’t even provide a syntax for the assignment operator to call the copy constructor. Conversely, it would compile to have the copy constructor call the assignment operator. But since the precondition for the copy constructor is an uninitialized object, the precondition for the assignment operator is an initialized object. Such a call is not a good design and may cause logical confusion.Use object to manage resourcesThose who are familiar with smart pointers will certainly not find this strange:{ // ... Binder *pBin = binder_factory.createBinder(); // ... delete pBin;}The C++ compiler does not provide an automated garbage collection mechanism, so it’s the programmer’s responsibility to release resources. We are always asked to use new and delete in pairs. The above code does work properly without leaking memory. However, the problem is that the createBinder() function shifts the responsibility of releasing the resource over to the caller, but does not explicitly declare this. Hence, the caller is sometimes unaware of it. Even if the caller knows that the resource needs to be destroyed, it may not be released in time due to flow control statements or exceptions.Fortunately, we can wrap the resource in an object and release it in a destructor. This eliminates the need for clients to maintain the resource’s memory. One such object is std::unique_ptr, called a smart pointer. A typical usage scenario where resources are stored in heap space but only used locally is:void f(){ std::unique_ptr&lt;Binder&gt; pBin(binder_factory.createBinder());}At the end of the f() call pBin exits the scope and the destructor is called, eventually causing the resource to be freed. It would be better to have createInvestment return a smart pointer directly. As you can see, the key to using objects to manage resources is to put them into a resource management object as soon as they are created, and to use the resource management object’s destructor to ensure that they are released.The framework for implementing the resource management object is exactly the RAII principle: acquisition is initialization, using a resource to initialize a smart pointer. The resource is released in the pointer’s destructor.With RAII, instead of writingstd::mutex mtx;void f(){ mtx.lock(); //... mtx.unlock();}we should be writingstd::mutex mtx;void f(){ std::lock_guard&lt;std::mutex&gt; guard(mtx); //...}ReferenceEffective C++" }, { "title": "Learning Java annotations", "url": "/posts/learning-java-annotations/", "categories": "Software", "tags": "Java, annotations", "date": "2022-01-02 22:34:00 -0500", "snippet": "OverviewJava developers must have used @Override annotation which declares that an instance method overrides a method of the same name and the same parameter type of the parent class. The definition of @Override annotation looks like the following:package java.lang;@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override {}Note that the @Override annotation itself is annotated by two other meta-annotations (i.e., annotations that act on the annotation). One of them, @Target, is used to limit the type that can be annotated by the target annotation, in this case @Override can only be used to annotate methods.@Retention is used to define the current annotation lifecycle. There are three different lifecycles for annotations: SOURCE: the annotation appears only in the source code CLASS: the annotation appears only in the source code and bytecode RUNTIME: the annotation appears only in the source code, bytecode and runtime.Annotaions’ lifecycle determines when they will be applied/used: SOURCE annotations like @Override are used by the compiler and are not compiled into the .class file. They are discarded after compilation. CLASS annotations are used by tools that handle .class files. These annotations are compiled into the .class file, but do not exist in memory after loading. Some underlying libraries use these annotations, and we generally do not have to deal with them ourselves. RUNTIME annotations are the most commonly used annotations. They can be read during the runtime of the program and are always present in the JVM after loading.Self defined annotationAs we know, Java’s annotation mechanism allows developers to customize annotations. Annotations do not affect the code logic. How annotations are used is entirely up to the tool. The compiler uses annotations of type SOURCE, so we generally only use them and do not implement them. Annotations of type CLASS are primarily used by the underlying tool library and involve class loading, which we rarely use. In practice, we almost only will implement self-defined RUNTIME type annotations.Note that all annotations inherit from java.lang.annotation. The methods provided by Java for reading Annotation using the reflection API include: Determining whether an annotation exists for a Class, Field, Method or Constructor: Class.isAnnotationPresent(Class) Field.isAnnotationPresent(Class) Method.isAnnotationPresent(Class) Constructor.isAnnotationPresent(Class) Get annotation: Class.getAnnotation(Class) Field.getAnnotation(Class) Method.getAnnotation(Class) Constructor.getAnnotation(Class) Use caseSuppose we want to parse tables from the relational database. The table might contain various data formats like date, text meg, integer array, floating-point numbers array etc. Tables are user input. Only user knows what formats are there in the tables and we don’t know what format is there beforehand. This means that we need to let the user bind the correct format parsers to the columns of the data table, and call the parser at runtime based on the column.We can expose a parser registry class that holds a map from column name to parser class, or we can utilize annotations which is a neat solution for this scenerio:@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface Processor { Class&lt;? extends Parser&gt; value();}public interface Parser { void setColumn(String column); void parse(String[][] rows);}public class DateParser implements Parser { private String columnName; public DateParser(String columnName) { this.columnName = columnName } // ...}public class IntArrayParser implements Parser { private String columnName; public IntArrayParser(String columnName) { this.columnName = columnName } // ...}public abstract class Table { // ...}public class TableLoader { public TableLoader() { // ... } public void load(Class tableClass, String fileName) { for (Field field : tableClass.getDeclaredFields()) { String columnName = field.getName(); if (field.isAnnotationPresent(Parser.class)) { Class&lt;? extends Parser&gt; parserClass = field.getDeclaredAnnotation(Parser.class).value(); Parser parser = parserClass.getConstructor(String.class).newInstance(columnName); // ... parse and other work } } if (field.isAnnotationPresent(Bind.class)) { Class&lt;? extends Parser&gt; parserClass = field.getDeclaredAnnotation(Bind.class).value(); }}// user codepublic class UserTable extends Table { @Parser(DateParser.class) Long birthday; @Parser(IntArrayParser.class) Integer[] account;}" }, { "title": "Common loop optimization techniques by compiler", "url": "/posts/common-loop-optimization-techniques-by-compiler/", "categories": "System", "tags": "compiler", "date": "2022-01-01 15:41:00 -0500", "snippet": "Before optimizing a loop, we need to first define a loop in the control flow graph. Note that each node in a control flow graph is a basic block.Identifying loopsCycles in the control flow graph are not necessary loops. A loop is a subset S of nodes where: S is strongly connected. In other words, for any two nodes in S, there is a path from one to the other using only nodes in S There is a distinguished header node h∈S such that there is no edge from a node outside S to S\\{h}DominatorsNode d dominates node n in a graph (d dom n) if every path fromthe start node to n goes through dDominators can be organized as a tree a -&gt; b in the dominator tree iff a immediately dominates bBack edgeBack edge is an edge from n to dominator d.ExampleIn the above example, we have: a dominates a,b,c,d,e,f,g,h b dominates b,c,d,e,f,g,h c dominates c,e d dominates d e dominates e f dominates f,g,h g dominates g,h h dominates h back-edges: g→b, h→aNatural LoopsThe natural loop of a back edge is the smallest set of nodes that includes the head and tail of the back edge, and has no predecessors outside the set, except for the predecessors of the header. Single entry-point: header that dominates all nodes in the loopAlgorithm to Find Natural Loops: Find the dominator relations in a flow graph Identify the back edges Find the natural loop associated with the back edgeLoop fusion &amp; loop fissionBoth loop fusion and loop fisson make good uses of the reference locality property. Loop fusion combines two loops into one while loop fission seperates one loop into two.Loop fusion// before fusionint sum = 0;for (int i = 0; i &lt; n; ++i) { sum += a[i]; a[i] = sum;}for (int i = 0; i &lt; n; ++i) { b[i] += a[i];}// after fusionint sum = 0;for (int i = 0; i &lt; n; ++i) { sum += a[i]; a[i] = sum; b[i] += a[i];}Loop fission// before fissionfor (int i = 0; i &lt; N; ++i) { a[i] = e1; b[i] = e2;}// after fissionfor (int i = 0; i &lt; N; ++i) { a[i] = e1;}for (int i = 0; i &lt; N; ++i) { b[i] = e2;}Loop unrollingLoop unrolling re-writes the loop body and each iteration of rewritten loop will perform several iterations of old loop.// before unrollingfor (int i = 0; i &lt; n; ++i) { a[i] = b[i] * 7 + c[i] / 13;}// after unrollingfor (int i = 0; i &lt; n; i+=3) { a[i] = b[i] * 7 + c[i] / 13; a[i + 1] = b[i + 1] * 7 + c[i + 1] / 13; a[i + 2] = b[i + 2] * 7 + c[i + 2] / 13;}The benefit of loop unrolling is reducing branching penalty &amp; end-of-loop-test costs.Loop tilingLoop tiling partitions a loop’s iteration space into smaller chunks or blocks, so as to help ensure data used in a loop stays in the cache until it is reused.// before tilingfor (i = 0; i &lt; n; i++) { c[i] = 0; for (j = 0; j &lt; n; j++) { c[i] = c[i] + a[i][j] * b[j]; }}// after tilingfor (i = 0; i &lt; n; i += 4) { c[i] = 0; c[i + 1] = 0; for (j = 0; j &lt; n; j += 4) { for (x = i; x &lt; min(i + 4, n); x++) { for (y = j; y &lt; min(j + 4, n); y++) { c[x] = c[x] + a[x][y] * b[y]; } } }}Loop interchage// before interchagefor (int j = 0; j &lt; n; ++j) { for (int i = 0; i &lt; n; ++i) { a[i][j] += 1; }}// after interchagefor (int i = 0; i &lt; n; ++i) { for (int j = 0; j &lt; n; ++j) { a[i][j] += 1; }}The benefit of loop interchange is reference locality.Loop parallelizationStrength reductionComputation HoistingReferenceWiki: Control Flow GraphWiki: Loop TilingCSC D70: Compiler Optimization LICM (Loop Invariant Code Motion)" }, { "title": "B+ tree as database index", "url": "/posts/B+-tree-as-database-index/", "categories": "Algorithm & Data Structure", "tags": "database, B+ tree", "date": "2021-12-30 22:49:00 -0500", "snippet": "What is a database index Index is a data structure to improve the speed of queries.Every book has its table of contents. Similarly, an index is the “table of contents” for a database table. The database index is there to improve data query efficiency.Why tree indexBefore answering this question, let’s consider implementing an index with common data structures that supports fast look-ups like hashmap, array and binary search tree.For hashmap, every insertion &amp; deletion will just be constant time, query by key will also be constant time, but query by the key range from [KEY_i, KEY_j] will require a linear look-up time.The array approach will have a amortized O(1) insertion time (append to tail). And both query by key and query by range will be O(1) if we map the table’s key into array index. However, the problem with array is that it doesn’t support dynamic insertion: insertion to the sorted keys will require linear time. Therefore, ordered array indexes are only suitable for static storage engines, i.e. storing data that will never be modified again.Binary search tree will have O(logN) query complexity. To maintain O(logN) query complexity, one needs to keep the tree as a balanced binary tree. The update time complexity is also O(logN). Assuming the ratio of query operations over update operations is 1:1, then a binary search tree seems like the optimal choice among the 3. However, most database stores use n-ary trees instead of binary trees in practice. The reason is that the indexes is stored in the disk, and each disk seek operation is costly. Using a binary tree will lead to a much bigger height for a large number of nodes than an N-ary tree when N is significant, which gives rise to a higher number of seek operations considering the height difference.B+ treeB+ tree indexes are among the most common index data structures in database systems and are supported by almost all relational databases. Why? Because it is by far the most efficient data structure for sorting. Binary trees, hash indexes and red-black trees are far less efficient than B+ tree indexes in terms of disk-based storage of large amounts of data.B+ tree indexes are characterized by a balanced tree based on disk, but the tree is very short, usually 3 to 4 levels, and can hold tens to hundreds of millions of sorted data. A short tree means less seeking, with only 3 or 4 I/Os to query data from tens or hundreds of millions of data. In addition, because the B+ tree is short, when doing the sorting, one only need to compare 3~4 times to locate the position where the data needs to be inserted, which is an excellent sorting efficiency.The B+ tree index consists of a root node, a non-leaf node, and a leaf node, where the leaf node holds all the sorted data.All B+ trees start with a tree of height 1 and then slowly increase as data is inserted. Note that a B+ tree index of height 1 holds already sorted records, so if we want to do another query within a leaf node, we can quickly locate the data by only doing a binary search.However, as more records are inserted into the B+ tree index, one page (16K) cannot hold so much data, so a splitting of the B+ tree occurs and the height of the B+ tree becomes 2. When the height of the B+ tree is greater than or equal to 2, the root and middle nodes hold index key pairs, consisting of (index key, pointer).As we can see, in the B+ tree index above, if we want to look up a record with index key value 5, we first look up the root node and find the key-value pair (5, address), which means that records less or equal to 5 are in the next level of leaf nodes pointed to by the address. Then we can find the leftmost leaf node according to the address of the next level, and we can find the record with index key value 5 in the leaf node according to the binary lookup.EfficiencyWhat is the theoretical maximum number of rows that a B+ tree index of height 2 can hold?In MySQL InnoDB storage engine, a page size is 16K, and let’s say the key-value pair userId is of type LONG, then the root node can hold at most the following: key-value pairs = 16K / key-value pair size (8+8) ≈ 1000Assuming again that the size of each record in the table is 500 bytes, then: The maximum number of records that can be stored in a leaf node is = 16K / size of each record ≈ 32In summary, the maximum number of records that can be stored in a B+ tree index with a tree height of 2 is: Total number of records = 1000 * 32 = 32,000In other words, after sorting 35,200 records, the resulting B+ tree index has a height of 2. Searching for a record based on the index key in 32,000 records requires only 2 pages, a root and a leaf node, to locate the page where the record is located.Similarly, the maximum number of records that can be stored in a B+ tree index with a tree height of 3 is: Total number of records = 1000 (root node) * 1000 (intermediate nodes) * 32 = 32,000,000We can conclude that: B+ tree indexes are typically 3 to 4 levels high, and a B+ tree of height 4 can hold about 5 billion records. Because of the low height of the B+ tree, queries are extremely efficient, and only 4 I/Os are needed to interpolate 5 billion records.Implementationnamespace data {struct Record { //...}}namespace index {struct Address { void* blockAddress; std::size_t offset;}class Node{private: Address *pointers; // A pointer to an array of struct {void *blockAddress, short int offset} containing other nodes in disk. float *keys; // Pointer to an array of keys in this node. int numKeys; // Current number of keys in this node. bool isLeaf; // Whether this node is a leaf node. friend class BPlusTree; // To access this class' private variables.public: Node(int maxKeys); // Takes in max keys in a node.};class BPlusTree{private: MemoryPool *disk; // Pointer to a memory pool for data blocks. MemoryPool *index; // Pointer to a memory pool in disk for index. Node *root; // Pointer to the main memory root (if it's loaded). void *rootAddress; // Pointer to root's address on disk. int maxKeys; // Maximum keys in a node. int levels; // Number of levels in this B+ Tree. int numNodes; // Number of nodes in this B+ Tree. std::size_t nodeSize; // Size of a node = Size of block. // Updates the parent node to point at both child nodes, and adds a parent node if needed. void insertInternal(float key, Node *cursorDiskAddress, Node *childDiskAddress); // Helper function for deleting records. void removeInternal(float key, Node *cursorDiskAddress, Node *childDiskAddress); // Finds the direct parent of a node in the B+ Tree. // Takes in root and a node to find parent for, returns parent's disk address. Node *findParent(Node *root, Node *node, float lowerBoundKey);public: // Constructor, takes in block size to determine max keys/pointers in a node. BPlusTree(std::size_t blockSize, MemoryPool *disk, MemoryPool *index); // Search for keys corresponding to a range in the B+ Tree given a lower and upper bound. Returns a list of matching Records. void search(float lowerBoundKey, float upperBoundKey); // Inserts a record into the B+ Tree. void insert(Address address, float key); // Remove a range of records from the disk (and B+ Tree). // Accepts a key to delete. int remove(float key);};}ReferenceB+ tree implementation" }, { "title": "Proxy design pattern and RPC framework", "url": "/posts/proxy-design-pattern-and-rpc-framework/", "categories": "Software", "tags": "design pattern, server, Java, Go, C++", "date": "2021-12-28 00:01:00 -0500", "snippet": " The proxy pattern is a structural design pattern that allows you to provide a substitute for an object or its placeholder. Proxies control access to the original object and allow some processing before and after the request is submitted to the object.Static proxypublic class ServerLoadBalancer { public Server find(Context context) { // return the least traffic server }}public class Server { public Response serve(Request request) { // ... }}public class ServerProxy extends Server { private ServerLoadBalancer loadBalancer; // ... public Response serve(Request request) { /* Do something before routing request to actual server The 'something' can be like network I/O, load balancing, service discovery etc. */ Context context = ... Server server = loadBalancer.find(context); server.serve(request) }}We can tell the drawbacks of static proxy (code above): we need to reimplement all the methods in the original class in the proxy class. Say there are over 100 classes to be proxied, then we need to make over 100 corresponding proxy classes. In addition, the code in each proxy class is a bit like boilerplate code, which adds unnecessary code maintainance costs.Apparently, a RPC framework cannot know the original class beforehand as the framework assumes no knowledge of the client(customer) class. So how can we solve this problem?Dynamic proxyWe can use dynamic proxies to solve this problem. Instead of writing a proxy class for each original class, we dynamically create a proxy class for the original class at runtime and then replace the original class with the proxy class in the framework.Note that dynamic proxy relies on Java’s reflection feature, namely the two core classes in the java.lang.reflect package: the InvocationHandler interface and the Proxy class.public interface InvocationHandler { public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;}public class Proxy { // ... public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) { Objects.requireNonNull(h); Class&lt;?&gt; caller = System.getSecurityManager() == null ? null : Reflection.getCallerClass(); Constructor&lt;?&gt; cons = getProxyConstructor(caller, loader, interfaces); return newProxyInstance(caller, cons, h); }}Each dynamic proxy object must provide an implementation class of the InvocationHandler interface, with only one invoke() method. When using a proxy object to invoke a method, the proxy object will eventually forward the method call to the invoke() method to execute the specific logic.The Proxy class is actually a factory class for dynamically creating proxy classes. It provides static method newProxyInstance() for dynamically generating the proxy classes.A working example utilizing dynamic proxy will be like the following:public interface LoggingService { void emit(Event event);}public class LoggingServiceImpl implements LoggingService { @Override public void emit(Event event) { Timestamp timestamp = ...; System.out.println(timestamp.toString() + event.toString()); }}public class LoggingServiceInvocationHandler implements InvocationHandler { private Object proxiedObject; public LoggingServiceInvocationHandler(Object proxiedObject) { this.proxiedObject = proxiedObject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"start logging\"); Object result = method.invoke(proxiedObject, args); System.out.println(\"emitted logging\"); return result; }}public class LoggingProxyFactory { private Object target; public LoggingProxyFactory(Object target) { this.target = target; } public Object getProxyInstance(Object proxiedObject) { LoggingServiceInvocationHandler handler = new LoggingServiceInvocationHandler(proxiedObject); return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), handler); }}RPC Frameworkpublic class RpcProtocol&lt;T&gt; implements Serializable { private RpcHeader header; private T body;}public class RpcRequest implements Serializable { private String serviceVersion; private String className; private String methodName; private Object[] params; private Class&lt;?&gt;[] parameterTypes;}public class RpcResponse implements Serializable { private Object data; private String msg;}public class RpcFuture&lt;T&gt; { private Promise&lt;T&gt; promise; private long timeout;}public class RequestQueue { private Queue&lt;RpcRequest&gt; queue; public RequestQueue() { queue = new ConcurrentLinkedQueue&lt;&gt;(); } public void hasRequest() { return queue.size() &gt; 0; } public void addRequest() { // ... } public void popRequest() { // ... }}public class RpcExecutor implements Runnable { //... public void start(RequestQueue queue) { while (queue.hasRequest()) { // ... } }}public class RpcInvocationHandler implements InvocationHandler { private RequestQueue queue; private RpcExecutor executor; private String serviceVersion; private long timeout; public RpcInvocationHandler(String serviceVersion, long timeout) { executor = new RpcExecutor(); queue = new RequestQueue(); this.serviceVersion = serviceVersion; this.timeout = timeout; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { RpcProtocol&lt;RpcRequest&gt; protocol = new RpcProtocol&lt;&gt;(); MsgHeader header = new MsgHeader(); // header.setMagic(...); // ... protocol.setHeader(header); RpcRequest request = new RpcRequest(); request.setServiceVersion(this.serviceVersion); request.setClassName(method.getDeclaringClass().getName()); request.setMethodName(method.getName()); request.setParameterTypes(method.getParameterTypes()); request.setParams(args); protocol.setBody(request); RpcFuture&lt;RpcResponse&gt; future = new RpcFuture&lt;&gt;(new DefaultPromise&lt;&gt;(new DefaultEventLoop()), timeout); queue.addRequest(protocol); return future.getPromise().get(future.getTimeout(), TimeUnit.MILLISECONDS).getData(); }}public class LoggingProxyFactory { private Object target; public LoggingProxyFactory(Object target) { this.target = target; } public Object getProxyInstance(Object proxiedObject) { LoggingServiceInvocationHandler handler = new LoggingServiceInvocationHandler(proxiedObject); return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), handler); }}AppendixGolang implementation of proxy pattern:// server.gotype server interface { handleRequest(string, string) (int, string)}// proxy.gotype proxy struct { application *application maxAllowedRequest int rateLimiter map[string]int}func newProxyServer() *proxy { return &amp;proxy{ application: &amp;application{}, maxAllowedRequest: 2, rateLimiter: make(map[string]int), }}func (p *proxy) handleRequest(url, method string) (int, string) { allowed := n.checkRateLimiting(url) if !allowed { return 403, \"Not Allowed\" } return p.application.handleRequest(url, method)}func (p *proxy) checkRateLimiting(url string) bool { if p.rateLimiter[url] == 0 { p.rateLimiter[url] = 1 } if p.rateLimiter[url] &gt; p.maxAllowedRequest { return false } p.rateLimiter[url] = p.rateLimiter[url] + 1 return true}// application.gotype application struct {}func (d *application) handleRequest(url, method string) (int, string) { if url == \"/app/status\" &amp;&amp; method == \"GET\" { return 200, \"Ok\" } if url == \"/create/user\" &amp;&amp; method == \"POST\" { return 201, \"User Created\" } return 404, \"Not Found\"C++ implementation of proxy pattern:// server interfaceclass Server { public: virtual int HandleRequest() const = 0;};class MyServer : public Server { public: int HandleRequest() const override { // ... return 1; }};class Proxy : public Server { private: MyServer *proxied_; public: Proxy(MyServer *proxied) : proxied_(new MyServer(*proxied)) { } ~Proxy() { delete proxied_; } void Request() const override { // ... this-&gt;proxied_-&gt;HandleRequest(); // ... }};void Demo(const Server &amp;server) { // ... server.HandleRequest(); // ...}" }, { "title": "Chain of responsibility design pattern", "url": "/posts/chain-of-responsibility-design-pattern/", "categories": "Software", "tags": "design pattern, Java, Go, C++", "date": "2021-12-20 00:01:00 -0500", "snippet": " The chain of responsibility design patternIn compiler backend development, we usually will have to run multiple passes on the intermediate representation. Those passes including optimization passes, lowering passes, scheduling passes and code gen passes. In addition, passes will follow some particular order, namely we always run optimization passes before scheduling and code gen passes. Inputs to the downstream passes usually will depend on the outputs of upstream passes with possibly some twisks.AppendixFollowing is a Go implementation of the chain of responsibility pattern.Following is a C++ example of the chain of responsibility pattern:" }, { "title": "Decorator design pattern and Java I/O package", "url": "/posts/decorator-design-pattern/", "categories": "Software", "tags": "design pattern, Java, Go, C++", "date": "2021-12-18 11:12:00 -0500", "snippet": " The decorator pattern is a way to dynamically add functionality to an instance of an object at runtime.If we want to read data from file with Java, our code will look something like the following:InputStream in = new FileInputStream(\"demo.txt\");InputStream bin = new BufferedInputStream(in);byte[] data = new byte[128];while (bin.read(data) != -1) { //...}We almost always need to do the following: Create a FileInputStream object first, and then pass it to BufferedInputStream object to use.Why not just have a BufferedFileInputStream class that inherits from FileInputStream and supports caching? This way we can create a BufferedFileInputStream object like the code below and open the file to read the data, wouldn’t it be easier to use?The truth is this approach is not scalable. Let’s say in addition to adding buffering function, we also want encryption and decryption support when transmitting data, as well as I/O from different data source like socket and pipe. 3 different final data sources with 2 functions will need 6 subclasses in total. If we continue to add functionalities &amp; data source support, the subclasses will explode.Instead of utilizing inheritance, utilizing composition will keep the class structure relatively simple. The following code shows how Java I/O accomplished this with decorator pattern:public abstract class InputStream { //... public int read(byte b[]) throws IOException { return read(b, 0, b.length); } public int read(byte b[], int off, int len) throws IOException { //... } public long skip(long n) throws IOException { //... } public void close() throws IOException {} //...}public class BufferedInputStream extends InputStream { protected volatile InputStream in; protected BufferedInputStream(InputStream in) { this.in = in; } //...}public class DataInputStream extends InputStream { protected volatile InputStream in; protected DataInputStream(InputStream in) { this.in = in; } //...}The purpose of the decorator pattern is to add additional functionalities to the original layer in a layering manner so that we can get the ultimate function we want by combining them.Interesting note: Python directly integrated decorator as a language feature.AppendixFollowing is a Golang example of the decorator pattern:// notifier interfacetype notifier interface { notify() bool}// default_notifier.gotype defaultNotifier struct {}func (d *defaultNotifier) notify() bool { // phone bannr push return success}// sms_notifier.gotype smsNotifier struct { notifier notifier}func (s *smsNotifier) notify() bool { success := s.notifier.notify() // ... // sms push return success}// email_notifier.gotype emailNotifier struct { notifier notifier}func (e *emailNotifier) notify() bool { success := e.notifier.notify() // ... // email push return success}// main.gofunc main() { notifier := &amp;defaultNotifier{} // add sms functionality notifierSms := &amp;smsNotifier{ notifier: notifier, } // add email functionality notifierSmsEmail := &amp;emailNotifier{ notifier: notifierSms, } notifierSmsEmail.notify()}Following is a C++ example of the decorator pattern:class Notifier { public: virtual ~Notifier() {} virtual bool Notify() const = 0;};class Decorator : public Notifier { protected: Notifier* notifier_; public: Decorator(Notifier* notifier) : notifier_(notifier) { } bool Notify() const override { return this-&gt;component_-&gt;Notify(); }};class SmsNotifier : public Decorator { public: SmsNotifier(Notifier* notifier) : Decorator(notifier) { } bool Notify() const override { // send SMS msg Decorator::Notify(); return 1; }};class EmailNotifier : public Decorator { public: SmsNotifier(Notifier* notifier) : Decorator(notifier) { } bool Notify() const override { // send email msg Decorator::Notify(); return 1; }};" }, { "title": "How deque is implemented in STL", "url": "/posts/how-deque-is-implemented-in-stl/", "categories": "Algorithm & Data Structure", "tags": "C++, STL", "date": "2021-12-17 17:41:00 -0500", "snippet": "A double-ended queue is open at both ends compared to the regular queue. Both push and pop operations can be performed at the head and tail of the queue. Essentially, a double-ended queue is just a queue that supports FIFO insertion and deletion at both ends and semantically does not support random access, insertion and deletion based on subscripts like an array.How to implement dequeIf we need to implement a fixed-length deque, we just need to allocate an array of size N. Two indices represent the head and tail. When there is an insertFront() operation, we do array[(--front+N)%N] = value. This implementation essentially constitutes a circular deque. And the add()/delete() operations are O(1) operations since all we need is to move the head/tail indices. We can even extend this solution to support random access by the element order.But the problem with this implementation is that if the deque is of dynamic length, each expansion will require linear time to copy the array.Suppose we implement deque in a linked list way. In that case, the add()/delete() operations are constant time, and we don’t need to consider the expansion of deque because each time we add an element, we only need to allocate one more node in memory. Such implementation looks lovely on the surface, but the problem with linked list is that each node in the chain may be discontinuous in memory. A cache line load may not read exactly the next node, and one main memory lookup is very expensive. Such implementation can lead to a degradation of deque performance.Is there a way to implement deque that supports expansion in O(1), and also makes use of locality pattern of memory access?How STL implements dequeIn STL, the memory layout of deque composes segments of contiguous space, and the address information of these spaces is stitched together with another array structure. The time complexity of insertion() and deletion() at the front and tail is O(1). For example, each time we use up an adjacent space, deque will request a new space and link it to the end of its segment space. So deque does not need to pay the high cost of replication and copying every time since we expand it like vector, nor does it need to request memory every time a new node is inserted like a linked list. Deque needs to keep a map to maintain a contiguous segment of memory space, which is called a buffer.template&lt;class T, class Ref, class Ptr, size_t BuffSize = 8&gt;struct __DequeIterator{\ttypedef T**\t\t\t\t\t\t\t\t\t\tMapPointer;\ttypedef __DequeIterator&lt;T, T&amp;, T*, BuffSize&gt;\tIterator;\ttypedef __DequeIterator&lt;T, Ref, Ptr, BuffSize&gt;\tSelf;\t//typedef __DequeIterator Self;\t// constructor\t__DequeIterator()\t:_cur(NULL)\t,_first(NULL)\t,_last(NULL)\t,_node(NULL)\t{}\t__DequeIterator(T* cur, MapPointer node)\t\t:_cur(cur)\t\t,_first(*node)\t\t,_last(_first + BuffSize)\t\t,_node(node)\t{}\t\t__DequeIterator(const Iterator&amp; s)\t\t:_cur(s._cur)\t\t,_first(s._first)\t\t,_last(s._last)\t\t,_node(s._node)\t{}\t// self increment\tSelf&amp; operator++()\t{\t\t++_cur;\t\tif (_cur == _last){\t\t\tSetNode(_node + 1);\t\t\t_cur = _first;\t\t}\t\treturn *this;\t}\tSelf operator++(int)\t{\t\tSelf temp(*this);\t\t++(*this);\t\treturn temp;\t} // self decrement\tSelf&amp; operator--()\t{\t\tif (_cur == _first){\t\t\tSetNode(_node - 1);\t\t\t_cur = _last;\t\t}\t\t--_cur;\t\t\t\treturn *this;\t}\tSelf operator--(int)\t{\t\tSelf temp(*this);\t\t--(*this);\t\treturn temp;\t}\t// comparison\tbool operator!=(const Self&amp; s)const\t{\t\treturn _cur != s._cur;\t}\tbool operator==(const Self&amp; s)const\t{\t\treturn !(operator!=(s));\t}\t// de-reference\tRef operator *()const\t{\t\treturn *_cur;\t}\tPtr operator-&gt;()const\t{\t\treturn &amp;(operator*());\t}\t// reset iterator position\tvoid SetNode(MapPointer newNode)\t{\t\t_node = newNode;\t\t_first = *newNode;\t\t_last = *newNode + BuffSize;\t}\tT* _cur;\tT* _first;\tT* _last;\tMapPointer _node;}Basically the iterator really has to pay attention to the buffer boudary, which brings us SetNode() method. Every time when iterator increments/decrements at the boudary, we SetNode() to next buffer (which also sets up _first and _last).template&lt;class T, size_t BuffSize = 8&gt;class Deque{\ttypedef T** MapPointer;public:\ttypedef __DequeIterator&lt;T, T&amp;, T*, BuffSize&gt;\t\t\t\tIterator;\ttypedef __DequeIterator&lt;T, const T&amp;, const T*, BuffSize&gt;\tConstIterator;\ttypedef T&amp; Ref;\ttypedef const T&amp; ConstRef;public:\t//construction\tDeque()\t\t:_map(NULL)\t\t,_mapSize(0)\t\t,_size(0)\t{\t\tassert(BuffSize &gt; 2);\t}\tvoid PushBack(const T&amp; value)\t{\t\tif (NULL == _map || _finish._cur == _finish._last - 1)\t\t\t\t_PushBackAux(value);\t\t\t\t\t\t\t\t\t\telse{\t\t\t*(_finish._cur) = value;\t\t\t++_finish._cur;\t\t\t++_size;\t\t}\t}\tvoid PushFront(const T&amp; value)\t{\t\tif (NULL == _map || _start._cur == _start._first)\t\t\t_PushFrontAux(value);\t\telse{\t\t\t*(_start._cur-1) = value;\t\t\t--_start._cur;\t\t\t++_size;\t\t}\t}\tvoid PopBack()\t{\t\t--_finish;\t\tif (_finish._cur == _finish._last){\t\t\tdelete[] * (_finish._node - 1);\t\t\t*(_finish._node - 1) = NULL;\t\t}\t\tif (_finish == _start){\t\t\tdelete[] _map;\t\t\t_map = NULL;\t\t}\t\t--_size;\t}\tvoid PopFront()\t{\t\t++_start;\t\tif (_start._cur == _start._first){\t\t\tdelete[] * (_start._node - 1);\t\t\t*(_start._node - 1) = NULL;\t\t}\t\tif (_finish == _start){\t\t\tdelete _map;\t\t\t_map = NULL;\t\t}\t\t--_size;\t}\t//Iteration\tIterator Begin()\t{\t\treturn _start;\t}\tConstIterator Begin()const\t{\t\treturn _start;\t}\tIterator End()\t{\t\treturn _finish;\t}\tConstIterator End()const\t{\t\treturn _finish;\t}\t//capacity\tsize_t Size()\t{\t\treturn _size;\t}\tbool Empty()\t{\t\treturn _start == _finish;\t}\tT&amp; Back()\t{\t\tassert(0 != _size);\t\tif (_finish._cur != _finish._first)\t\t\treturn *(_finish._cur - 1);\t\telse{\t\t\tIterator it(_finish);\t\t\t--it;\t\t\treturn *(it._cur);\t\t}\t}\tT&amp; Front()\t{\t\tassert(0 != _size);\t\treturn *(_start._cur);\t}\t~Deque() // destructor to release memory\t{\t\tif (_map){\t\t\tT** cur = _start._node;\t\t\tfor (; cur != _finish._node; ++cur){\t\t\t\tif (*cur){\t\t\t\t\tdelete[] * cur;\t\t\t\t\t*cur = NULL;\t\t\t\t}\t\t\t}\t\t\tif (*cur){\t\t\t\tdelete[] * cur;\t\t\t\t*cur = NULL;\t\t\t}\t\t\tdelete[] _map;\t\t\t_map = NULL;\t\t}\t}protected:\tvoid _PushBackAux(const T&amp; value)\t{\t\tif (NULL == _map || _map + _mapSize - 1 == _finish._node){\t\t\tsize_t newSize = _mapSize == 0 ? 2 : _mapSize * 2;\t\t\tMapPointer temp = new T*[newSize]; // allocate new room\t\t\tfor (size_t i = 0; i &lt; newSize; ++i)\t\t\t\t*(temp + i) = NULL;\t\t\t\t\t\tsize_t addToNode = _mapSize / 2;\t//0\t\t\tfor (size_t i = 0; i &lt; _mapSize; ++i)\t\t\t\ttemp[addToNode + i] = _map[i];\t\t\tsize_t oldStartNode = _start._node - _map;//0\t\t\tsize_t oldFinishNode = _finish._node - _map;//0\t\t\tif (_map)\t\t\t\tdelete[] _map;\t\t\t_map = temp;\t\t\tif (NULL != _finish._cur){\t\t\t\t_start.SetNode(temp + addToNode + oldStartNode);\t\t\t\t_finish.SetNode(temp + addToNode + oldFinishNode);\t\t\t}\t\t\telse{\t\t\t\t*(_map) = new T[BuffSize];\t\t\t\t_finish.SetNode(temp);\t\t\t\t_start.SetNode(temp);\t\t\t\t_finish._cur = *(_map) + BuffSize / 2;\t\t\t\t_start._cur = *(_map) + BuffSize / 2;\t\t\t\t*(_finish._cur++) = value;\t\t\t\t++_size;\t\t\t\t_mapSize = newSize;\t\t\t\treturn;\t\t\t}\t\t\t_mapSize = newSize;\t\t}\t\t*(_finish._cur) = value;\t\t*(_finish._node + 1) = new T[BuffSize];\t\t_finish.SetNode(_finish._node + 1);\t\t_finish._cur = _finish._first;\t\t++_size;\t}\tvoid _PushFrontAux(const T&amp; value)\t{\t\tif (NULL == _map || _map == _start._node){\t\t\tsize_t newSize = _mapSize == 0 ? 2 : _mapSize * 2;\t\t\tMapPointer temp = new T*[newSize];\t\t\tsize_t addToNode = _mapSize / 2;\t\t\tfor (size_t i = 0; i &lt; _mapSize; ++i)\t\t\t\ttemp[addToNode + i] = _map[i];\t\t\tsize_t oldStartNode = _start._node - _map;\t\t\tsize_t oldFinishNode = _finish._node - _map;\t\t\tif (_map)\t\t\t\tdelete[] _map;\t\t\t_map = temp;\t\t\tif (NULL != _start._cur){\t\t\t\t_start.SetNode(temp + addToNode + oldStartNode);\t\t\t\t_finish.SetNode(temp + addToNode + oldFinishNode);\t\t\t}\t\t\telse{\t\t\t\t*(_map) = new T[BuffSize];\t\t\t\t_start.SetNode(_map);\t\t\t\t_finish.SetNode(_map);\t\t\t\t_start._cur = *(_map)+BuffSize / 2;\t\t\t\t_finish._cur = *(_map)+BuffSize / 2;\t\t\t\t*(_start._cur - 1) = value;\t\t\t\t--_start._cur;\t\t\t\t++_size;\t\t\t\t_mapSize = newSize;\t\t\t\treturn;\t\t\t}\t\t\t_mapSize = newSize;\t\t}\t\t*(_start._node - 1) = new T[BuffSize];\t\t_start.SetNode(_start._node - 1);\t\t_start._cur = _start._last - 1;\t\t*(_start._cur) = value;\t\t++_size;\t}protected:\tMapPointer _map;\tIterator _start;\tIterator _finish;\tsize_t _mapSize;\tsize_t _size;};" }, { "title": "JVM execution engine - interpreter and JIT", "url": "/posts/jvm-execution-engine-interpreters-and-jit/", "categories": "System", "tags": "JIT, JVM, Java", "date": "2021-12-16 14:33:00 -0500", "snippet": "Common compiled languages such as C++ usually compile the code directly into machine code that the CPU understands to run. On the other hand, to achieve the “compile once, run everywhere” feature, Java divides the compilation process into two parts to execute.How JVM executes JAVA codeUsually, the JVM contains two core modules: the executor and the memory manager, where the executor is specifically used to execute the bytecode. The most widely used virtual machine is Hotspot, whose executor includes an interpreter and a JIT compiler.Before interpreter can start executing Java code, the first step is to compile the source code into byte code through javac. This process includes lexical analysis, syntax analysis, semantic analysis. Next, the interpreter directly interpretes bytecode and executes line by line without compilation. In the meantime, the virtual machine collects meta-data regarding the program’s execution. The compiler (JIT) can gradually come into play based on this data. It will perform backstage compilation - compiling the bytecode into machine code. But JIT will only compile code identified as a hotspot by the JVM.Let’s look at an example.import java.lang.Math;public class ByteCodeDemo { public static int absDifference(int a, int b) { int difference = a - b; return Math.abs(difference); } public static void main(String[] args) { System.out.println(absDifference(2, 1)); }}One can use the javap command to see its bytecode:&gt;&gt;&gt; javac ByteCodeDemo.java&gt;&gt;&gt; javap -c ByteCodeDemo.classCompiled from \"ByteCodeDemo.java\"public class ByteCodeDemo { public ByteCodeDemo(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 4: return public static int absDifference(int, int); Code: 0: iload_0 1: iload_1 2: isub 3: istore_2 4: iload_2 5: invokestatic #2 // Method java/lang/Math.abs:(I)I 8: ireturn public static void main(java.lang.String[]); Code: 0: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 3: iconst_2 4: iconst_1 5: invokestatic #4 // Method absDifference:(II)I 8: invokevirtual #5 // Method java/io/PrintStream.println:(I)V 11: return}Each bytecode has its op_code with either 0 or 1 argument. Each op_code is an unsigned byte type integer in the class file, occupying precisely one byte, which is how JVM instructions are called bytecodes. Note that the javap command translates the op_code into literal helper characters for human readability.We can take a look at the bytecode of the absDifference() method. The number 0 preceding iload_0 represents the offset of this bytecode. The number 1 at the next line also illustrates the offset value of this bytecode. If we look at bytecode invokestatic, we can notice that this bytecode is different from the previous one as it has a parameter #2 and a length of 3 bytes.So how does the interpreter work? The interpreter is in fact a Stack Machine that executes bytecode in stack order according to the semantics of the bytecode. Take the above substraction for example. When interpreter executes a substraction, it will first push two operands into stack, in our case is iload_0 and iload_1. Then it exeutes isub which will pop operand 0 and operand 1 out of stack and perform the substraction. Then it executes istore_2 which pushes the substraction result into the stack.JITThere are two core mechanisms that JIT compilers rely on, which are: Request writable and executable memory areas to ensure that executable machine code can be generated during runtime. Profiling Guided Optimization, which allows the JIT compiler to achieve runtime performance that exceeds that of static compilers.Specifically, the first mechanism is to request a memory block with both write and execute permissions. Then, compile the Java method and write the compiled machine code to the requested memory block. When the original Java method is called, instead of interpreting the method, JIT directly call the executable memory block.The second mechanism involves runtime profiling. Basically JVM uses two counters to profile, the Invocation Counter and the Back Edge Counter. Invocation Counter: used to count the number of times a method has been called Back Edge Counter: used to count the number of times the loop code is executed in a method, and the instruction that jumps backward in the bytecode is called Back Edge.JVM will trigger JIT compilation whenever the two counter goes above some pre-set threshold. Those over threshold code is considered HotSpot." }, { "title": "Pub-Sub design pattern", "url": "/posts/pub-sub-design-pattern/", "categories": "Software", "tags": "design pattern, Java, Go, C++", "date": "2021-12-15 21:48:00 -0500", "snippet": "Pub-Sub design pattern is usually known as Observer pattern. I personally like to refer this pattern as Pub-Sub which I think better captures the essence of this pattern.In GoF’s book Design Patterns, it is defined as follows: Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automaticallyLet’s look at the classical implementation of this design pattern.public interface Subject { void registerObserver(Observer observer); void removeObserver(Observer observer); void notifyObservers(Message message);}public interface Observer { void update(Message message);}public class ConcreteSubject implements Subject { private List observers = new ArrayList(); @Override public void registerObserver(Observer observer) { observers.add(observer); } @Override public void removeObserver(Observer observer) { observers.remove(observer); } @Override public void notifyObservers(Message message) { for (Observer observer : observers) { observer.update(message); } }}public class ConcreteObserverOne implements Observer { @Override public void update(Message message) { //TODO obtain notifications and process System.out.println(\"ConcreteObserverOne is notified.\"); }}public class ConcreteObserverTwo implements Observer { @Override public void update(Message message) { //TODO obtain notifications and process System.out.println(\"ConcreteObserverTwo is notified.\"); }Let’s take a look at a concrete example and see how this pattern can help. Suppose we are developing a workflow management system. Whenever a workflow is created, we need to register the configuration of the workflow into elastic storage and push workflow into execution queue. We can implement this like the following:public class WorkflowManager { private StorageService storageService; // dependency injection private ExecutionQueue executionQueue; // dependency injection public void register(Workflow workflow) { // ... WorkflowConfig config = workflow.getConfiguration(); storageService.save(config); executionQueue.push(workflow); // ... }}Now suppose, say after a workflow is registered, we put it on hold and notify the creator that the workflow has been validated and created and is ready to run instead of putting it into execution. In this case, we need to frequently modify the register() function code, which violates the open-close principle. Moreover, suppose a successful registration will require more and more follow-up operations. In that case, the logic of the register() function will become more and more complex, which will affect the readability and maintainability of the code. This is where the observer pattern comes in handy. Using the observer pattern, we can refactor the above code like the following:public interface RegObserver { void onRegSuccess(Workflow workflow);}public class RegStorageObserver implements RegObserver { private StorageService storageService; @Override public void onRegSuccess(Workflow workflow) { WorkflowConfig config = workflow.getConfiguration(); storageService.save(config); }}public class RegNotificationObserver implements RegObserver { private NotificationService notificationService; @Override public void onRegSuccess(Workflow workflow) { Contact owner = workflow.getOwner(); notificationService.sendMail(owner, \"Workflow is ready...\"); }}public class WorkflowManager { private WorkflowValidator validator; private List&lt;RegObserver&gt; regObservers = new ArrayList&lt;&gt;(); public void setRegObservers(List&lt;RegObserver&gt; observers) { regObservers.addAll(observers); } public void register(Workflow workflow) { if (!validator.validate(workflow)) { // ... } for (RegObserver observer : regObservers) { observer.onRegSuccess(workflow); } }}AppendixFollowing is a Golang implementation of Pub-Sub pattern:// subject.gotype subject interface { registerObserver(Observer observer) removeObserver(Observer observer) notifyObservers(string)}// event.go implements subjecttype event struct { observers []observer name string}func newEvent(name string) *event { return &amp;event{ name: name, }}func (e *event) registerObserver(o observer) { i.observers = append(e.observers, o)}func (e *event) removeObserver(o observer) { i.observerList = removeFromslice(e.observers, o)}func (e *event) notifyObservers() { for _, observer := range e.observers { observer.update(e.name) }}// observer.gotype observer interface { update(string)}// listener.go implements observertype listener struct { id string}func (l *listener) update(eventMsg string) { fmt.Printf(\"Listener %s received event %s\\n\", l.id, eventMsg)}Following is a C++ example of the Pub-Sub pattern:" }, { "title": "How to design a key-value storage system?", "url": "/posts/how-to-design-a-key-value-storage-system/", "categories": "System", "tags": "system design, key-value store", "date": "2021-12-14 01:09:00 -0500", "snippet": " The following are my notes from reading G.K’s system design bookLet’s first condier a easier question: how to design a simple key-value storage system on a single machine?Single node key-value storeThe most straightforward way is to use a hash table to store key-value pairs, which is how most such systems work today. Hash tables allow us to read/write a key-value pair in constant time and are very easy to implement as most languages have built-in support for it.However, the drawbacks are also evident. Using hash tables usually means storing everything in memory, which may not be possible when the data set is large. There are two standard solutions. Compress our data. This should be the first thing to consider, and there are often many things we can compress. For example, we can store references instead of the actual data. We can also use float32 instead of float64. Also, it is valid to use a different data representation like bit arrays (integers) or vectors. Storage on disk. If it is impossible to put everything in memory, you can store some data on a disk, which means we can think of this system as a caching system to optimize it further: we keep the frequently accessed data in memory and the rest on disk.Distributed key-value storeThe exciting part is the scaling of key-value stores to multiple machines. Since one machine does not have enough storage space for all the data, the general idea is to split the data across multiple machines by some rules. The coordinator can direct the clients to the machine with the requested resources. The question is how to split the data across multiple machines and, more importantly, what is the strategy for distributing the data?SplittingSuppose all the keys are URLs like and we have 26 machines. One way to do this is to assign all keys (URLs) to these 26 machines based on the first character of the URL (after www). For example, https://google.ca would be stored on machine G, while https://jiahuanglin.xyz will be stored on machine J. So what is the downside of this design?Let’s ignore the case where the URL contains ASCII characters. A good sharding algorithm should balance the traffic evenly across all machines. In other words, ideally, each machine would receive the same number of requests. The above design is not good. First of all, the storage is not evenly distributed. There are probably more URLs starting with a than z. Second, some URLs are more popular, such as sites like Facebook and Google.It is better to make sure that the keys are randomly distributed to balance the traffic. Another solution is to use a hash of URLs, which usually performs better.System availabilitySystem availability is an important metric to evaluate a distributed system. For example, suppose one of our computers crashes for some reason (perhaps a hardware problem or a program error). How does this affect our key-value storage system?If someone requests resources from this machine, we will not be able to return the correct response. Crashes will often happen if we use a large number of servers to serve millions of users, and you won’t be able to restart the server every time manually. That’s why availability is essential in every distributed system today. So how can we solve this problem?Of course, we can write more robust code with test cases. But there will always be bugs in our programs. In addition, hardware problems are more challenging to protect. The most common solution is redundancy. By creating machines with duplicate resources, we can significantly reduce system downtime. If one machine has a 10% chance of crashing every month, then using a backup machine reduces the probability of both machines being down to 1%.Redundancy versus shardingAt first glance, redundancy and sharding look very similar. So how do the two relate? How do we choose between redundancy and sharding when designing a distributed key-value store?Note that Sharding is basically used to split data across multiple machines because one machine cannot store too much data. Redundancy is a way to protect the system from downtime. With that in mind, redundancy is useless if one machine can’t hold all the data.By introducing redundancy, we can make the system more robust. However, consistency is an issue. For example, if machine M1 exists and has redundancy M2, how do you ensure that M1 and M2 have the same data? For example, we need to update both machines when inserting a new entry. But one of the write operations may fail. So over time, M1 and M2 may have a lot of inconsistent data, which is a big problem.Here are a couple of solutions. The first method is to keep the local copy in the coordination machine. Whenever a resource is updated, the coordinator keeps a copy of the updated version. Therefore, if the update fails, the coordinator can operate to update.The other method is to commit the log. Each node machine keeps a commit log of each operation, just like the history of all updates. So when we want to update an entry in machine M, it will first store that request in the commit log. Then we can have a separate program will process all the commit logs in order (in a queue). Whenever an operation fails, we can easily recover because we can look up the commit log.The last method is to resolve conflicts in reads. Suppose the coordinator can request all three machines when the requested resources are located in M1, M2 and M3. If the coordinator sees the data is different, coordinator can resolve the conflict instantly.Read throughputTypically, key-value storage systems should support a large number of reading requests. So, what methods can we apply to increase read throughput?Utilizing memory is a common approach to improve read throughput. If the data is stored on a disk in each node machine, we can move some of it to memory. The more common idea is to use caching." }, { "title": "Simple implementation of a logging framework", "url": "/posts/simple-implementation-of-a-logging-framework/", "categories": "Software", "tags": "logging, framework, server, Java", "date": "2021-12-14 00:16:00 -0500", "snippet": "Details of an Apache Log4j vulnerability were recently made public, which attackers could exploit to execute code remotely. The matter has caused an extremely heated discussion online and has led to countless programmers working overtime to change the code. Numerous software is implemented in java, each equipped with a logging system that depends on log4j. The silly-sounding question is, why do we need a logging module?Why do we need loggingWhen we talk about software development, we refer to a pipeline: development, testing, and then release to production. No one can write bug-free code or tests that cover every single corner case all the time. Usually, programmers use debuggers or simply just printf to debug programs. However, when we launched the server in the production environment, we could no longer troubleshoot problems through debuggers, mainly because we couldn’t just hang the execution and leave the user’s request stuck there. In addition, the production environment is typically distributed/multi-processed, which in general will make debugger useless.Since we can’t debug through a debugger, we need a logging system to track and recall the program’s behaviour and locate the problem.Functionalities of a logging frameworkGenerally speaking, a logging system will support the following functionalities over a simple printf: Supports hierarchy of messages — namely, DEBUG, INFO, WARNING, ERROR, etc. So developers don’t need to comment out the output statements after development but only adjust the level. Support for multiple output destinations. For example, we can have a log printed on the screen and save it to a file in the meantime for persistent storage and easy analysis. Support for log scrolling. When there are too many log files, the log library can delete some obsolete and irrelevant logs. Better performance. Compared with printf, log libraries are usually highly optimized for formatting and output performance, not affecting the program’s execution efficiency. Multi-threaded processing. With printf, when multiple threads are outputting simultaneously, a single line of logs may mix up the output of numerous threads, and the log library can synchronize the work correctly and efficiently to avoid output confusion.How to implement a logging modulepublic enum LoggingLevel { DEBUG(1, \"DEBUG\"), INFO(2, \"INFO\"), WARNING(3, \"WARNING\"), ERROR(4, \"ERROR\"); private int level; private String name; public LoggingLevel(int level, String name) { this.level = level; this.name = name; } public String toString() { return name; }}public class LoggingEvent { public long timestamp; private LoggingLevel level; private Object message; private String threadName; private long threadId; private String loggerName; public LoggingEvent withLevel(LoggingLevel level) { this.level = level; return this; } public LoggingEvent withMessage(Object message) { this.message = message; return this; } public LoggingEvent withLoggerName(String loggerName) { this.loggerName = loggerName; return this; } // ... public String toString() { return String.format( \"[%s-%d]-[%s] %s, thread %s, threadID %d\", loggerName, timestamp, level, message, threadName, threadId ) }}public interface Writer { void emit(LoggingEvent event);}public class ConsoleWriter implements Writer { private OutputStream out = System.out; @Override public void emit(LoggingEvent event) { try { out.write(event.toString().getBytes(encoding)); } catch (IOException e) { e.printStackTrace(); } }}public interface Logger { void info(String msg); void debug(String msg); void warning(String msg); void error(String msg); String getName(); }public class DefaultLogger implements Logger{ private String name; private Writer writer; private LoggingLevel level = Level.INFO; private int outputLevel; @Override public void info(String msg) { filterAndLog(Level.INFO, msg); } @Override public void debug(String msg) { filterAndLog(Level.DEBUG, msg); } @Override public void warning(String msg) { filterAndLog(Level.WARN, msg); } @Override public void error(String msg) { filterAndLog(Level.ERROR, msg); } private void filterAndLog(LoggingLevel level, String msg){ LoggingEvent e = new LoggingEvent() .withLevel(level) .withMessage(meg) .withLoggerName(getName()); if(level.toInt() &gt;= outputLevel){ writer.emit(e); } } @Override public String getName() { return name; } //...}" }, { "title": "Flood-fill algorithm", "url": "/posts/flood-fill-algorithm/", "categories": "Algorithm & Data Structure", "tags": "flood-fill, graphics", "date": "2021-12-07 11:01:00 -0500", "snippet": "" }, { "title": "Configuring the runtime parameters for server program", "url": "/posts/configuring-the-runtime-parameters-for-server-program/", "categories": "Software", "tags": "server", "date": "2021-12-03 20:55:00 -0500", "snippet": "Although we can pass argv through the main function, it is still tedious when dealing with server applications with lots of parameters that need to be specified flexibly. As a result, server programs typically support the following approaches for loading configurations in order to start: Passing through cmd line Configuration of environment variables Configuration of files Pull from remote configuration center Hard-coded default values in the programNotice that different configuration approaches will have different priorities to allow flexible overrides. Usually, the command line parsing will have top priority, and environment variable configuration will come second over other approaches. The higher priority configuration approach overrides the lower priority configuration approach parameters with the same name.Command-line parsing is commonly used during debugging to override particular settings without altering the configuration file quickly.Environment variables are also used to override specific parameters but avoid lengthy command-line parameters. In addition, configuring the program through environment variables is a standard configuration method when deploying containerized services.File configuration is usually used for stable program configuration after officially launching the program. One of the benefits of using files is that it is easy to view and modify. Also, the program can detect changes to the configuration file while running. The new configuration can take effect without restarting the program after modifying the configuration file, making the configuration changes have minimal impact on the online service.In contrast to the file configuration, pulling from a remote configuration centre is a unified service responsible for issuing program settings in a distributed system and guaranteeing that every server program uses the same configuration. As with file configuration, the program can listen for changes to the configuration center or have the configuration center proactively issue new configurations, enabling configuration reloading." }, { "title": "Scanline algorithm", "url": "/posts/scanline-algorithm/", "categories": "Algorithm & Data Structure", "tags": "scanline, graphics", "date": "2021-12-01 11:01:00 -0500", "snippet": "" } ]
